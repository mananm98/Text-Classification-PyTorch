{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b855300",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/manansinghmehta/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834dfbea",
   "metadata": {},
   "source": [
    "# 1. Dataset Generation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1080dc",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2297845",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/manansinghmehta/miniforge3/envs/tf_m1/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3441: DtypeWarning: Columns (7) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_table('/Users/manansinghmehta/Downloads/amazon_reviews_us_Beauty_v1_00.tsv',on_bad_lines='skip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de4b1e2",
   "metadata": {},
   "source": [
    "## Keep Reviews and Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ece8394",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_body</th>\n",
       "      <th>star_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Love this, excellent sun block!!</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The great thing about this cream is that it do...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Great Product, I'm 65 years old and this is al...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I use them as shower caps &amp; conditioning caps....</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This is my go-to daily sunblock. It leaves no ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         review_body star_rating\n",
       "0                   Love this, excellent sun block!!           5\n",
       "1  The great thing about this cream is that it do...           5\n",
       "2  Great Product, I'm 65 years old and this is al...           5\n",
       "3  I use them as shower caps & conditioning caps....           5\n",
       "4  This is my go-to daily sunblock. It leaves no ...           5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[['review_body','star_rating']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45aff687",
   "metadata": {},
   "outputs": [],
   "source": [
    "#some start_ratings have dates removing those rows\n",
    "df = df[ df['star_rating'].apply(lambda x: (isinstance(x, str) and len(x) == 1) or isinstance(x, int))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18dae6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping nan values from df\n",
    "df.dropna(axis = 0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb0efbea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_body</th>\n",
       "      <th>star_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Love this, excellent sun block!!</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The great thing about this cream is that it do...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Great Product, I'm 65 years old and this is al...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I use them as shower caps &amp; conditioning caps....</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This is my go-to daily sunblock. It leaves no ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         review_body  star_rating\n",
       "0                   Love this, excellent sun block!!            3\n",
       "1  The great thing about this cream is that it do...            3\n",
       "2  Great Product, I'm 65 years old and this is al...            3\n",
       "3  I use them as shower caps & conditioning caps....            3\n",
       "4  This is my go-to daily sunblock. It leaves no ...            3"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mappings = {'1':1,\n",
    "            '2':1,\n",
    "            '3':2,\n",
    "            '4':3,\n",
    "            '5':3,\n",
    "            1:1,\n",
    "            2:1,\n",
    "            3:2,\n",
    "            4:3,\n",
    "            5:3\n",
    "           }\n",
    "\n",
    "df['star_rating'] = df['star_rating'].apply(lambda x : mappings[x])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d724b1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_1 = df[ df['star_rating'] == 1 ].sample(n = 20000)\n",
    "class_2 = df[ df['star_rating'] == 2 ].sample(n = 20000)\n",
    "class_3 = df[ df['star_rating'] == 3 ].sample(n = 20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb1074d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_body</th>\n",
       "      <th>star_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Does anti-cellulite cream even exist?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It is not water resistant like it said it was....</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Service good. Product to pricy to purchase. Ma...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>so far i dont know if it is working so we shal...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I had given this product a great rating on ano...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         review_body  star_rating\n",
       "0              Does anti-cellulite cream even exist?            1\n",
       "1  It is not water resistant like it said it was....            2\n",
       "2  Service good. Product to pricy to purchase. Ma...            1\n",
       "3  so far i dont know if it is working so we shal...            2\n",
       "4  I had given this product a great rating on ano...            2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([class_1, class_2,class_3], ignore_index=True)\n",
    "df = df.sample(frac = 1)\n",
    "df = df.reset_index(drop= True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc336f0",
   "metadata": {},
   "source": [
    "# 2. Word Embedding (25 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76e5f2d",
   "metadata": {},
   "source": [
    "## (a) Load the pretrained Word2Vec model and check semantic similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5aac0cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f029b9b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 — </s>\n",
      "1 — in\n",
      "2 — for\n",
      "3 — that\n",
      "4 — is\n"
     ]
    }
   ],
   "source": [
    "for index, word in enumerate(wv.index_to_key):\n",
    "    if index == 5:\n",
    "        break\n",
    "    print(f\"{index} — {word}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285faf89",
   "metadata": {},
   "source": [
    "### Sample Word Embedding Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f3bf82d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01226807,  0.06225586,  0.10693359,  0.05810547,  0.23828125,\n",
       "        0.03686523,  0.05151367, -0.20703125,  0.01989746,  0.10058594,\n",
       "       -0.03759766, -0.1015625 , -0.15820312, -0.08105469, -0.0390625 ,\n",
       "       -0.05053711,  0.16015625,  0.2578125 ,  0.10058594, -0.25976562,\n",
       "        0.078125  ,  0.16113281,  0.12890625, -0.00318909,  0.00744629,\n",
       "        0.10693359, -0.01696777, -0.22070312,  0.01239014,  0.08154297,\n",
       "       -0.12158203,  0.24707031, -0.08105469, -0.21191406,  0.12695312,\n",
       "        0.07568359,  0.12792969,  0.11035156,  0.09863281,  0.24316406,\n",
       "        0.21875   ,  0.09716797,  0.13671875,  0.04272461, -0.05151367,\n",
       "       -0.01257324, -0.11083984, -0.17089844,  0.07470703,  0.15625   ,\n",
       "        0.04101562,  0.26953125,  0.01037598, -0.09814453,  0.10595703,\n",
       "       -0.3203125 ,  0.10888672, -0.09228516,  0.05517578, -0.00756836,\n",
       "       -0.03344727,  0.12207031, -0.07324219,  0.04467773,  0.046875  ,\n",
       "        0.05957031,  0.06225586,  0.11035156,  0.24023438,  0.29296875,\n",
       "        0.25976562, -0.22753906, -0.01495361,  0.06079102, -0.01000977,\n",
       "       -0.26171875,  0.01965332, -0.12109375,  0.04174805,  0.08837891,\n",
       "       -0.12597656,  0.03881836,  0.24316406,  0.1171875 ,  0.14941406,\n",
       "       -0.01477051, -0.1171875 ,  0.01745605,  0.08789062,  0.10107422,\n",
       "       -0.01208496,  0.03442383, -0.07080078,  0.04052734,  0.140625  ,\n",
       "       -0.19335938,  0.13378906, -0.03930664, -0.28125   , -0.19628906,\n",
       "       -0.15332031,  0.11132812,  0.13476562, -0.0612793 , -0.14160156,\n",
       "       -0.27539062, -0.06445312, -0.21191406, -0.08300781,  0.05737305,\n",
       "        0.14550781, -0.04248047, -0.23046875, -0.09521484,  0.20410156,\n",
       "       -0.00616455,  0.05395508, -0.00280762,  0.09472656, -0.11816406,\n",
       "        0.04003906, -0.16894531, -0.04101562,  0.42382812, -0.11279297,\n",
       "       -0.19824219, -0.15820312, -0.05932617, -0.16992188,  0.08496094,\n",
       "        0.03320312, -0.21875   , -0.05664062, -0.08154297, -0.10302734,\n",
       "       -0.18457031,  0.15234375,  0.21679688, -0.31445312, -0.05859375,\n",
       "       -0.00872803, -0.09912109,  0.15332031,  0.01867676, -0.0703125 ,\n",
       "        0.32421875, -0.00244141, -0.03930664, -0.23535156,  0.02734375,\n",
       "       -0.03088379, -0.01611328, -0.19824219,  0.125     , -0.07080078,\n",
       "        0.01531982, -0.08837891, -0.3203125 ,  0.05761719, -0.21679688,\n",
       "        0.01293945,  0.125     ,  0.05932617, -0.04199219,  0.00263977,\n",
       "       -0.11328125,  0.22167969, -0.17773438,  0.00149536, -0.01513672,\n",
       "       -0.3515625 , -0.20214844,  0.10107422,  0.02734375, -0.00817871,\n",
       "        0.03979492, -0.09570312, -0.26757812, -0.25585938,  0.02380371,\n",
       "        0.03979492, -0.07763672,  0.11376953,  0.11376953, -0.04467773,\n",
       "        0.10644531, -0.13183594, -0.07714844,  0.06787109, -0.19628906,\n",
       "        0.06298828, -0.05151367,  0.10107422, -0.04248047,  0.00982666,\n",
       "       -0.08447266,  0.0222168 , -0.19726562, -0.10009766, -0.01470947,\n",
       "        0.03637695, -0.05688477,  0.05029297, -0.078125  ,  0.10693359,\n",
       "       -0.11328125,  0.11132812, -0.06445312,  0.04931641,  0.01965332,\n",
       "        0.18457031,  0.35742188,  0.00793457,  0.20898438, -0.22167969,\n",
       "       -0.07763672,  0.00457764, -0.13964844, -0.2578125 , -0.25390625,\n",
       "        0.05541992, -0.15234375,  0.12060547, -0.13671875,  0.10986328,\n",
       "        0.08398438, -0.01916504,  0.06030273,  0.09521484, -0.10253906,\n",
       "       -0.1015625 ,  0.10839844, -0.17773438, -0.12695312,  0.296875  ,\n",
       "       -0.03710938, -0.12988281, -0.13671875,  0.13183594, -0.08300781,\n",
       "       -0.11230469,  0.01495361,  0.13183594, -0.24121094, -0.00159454,\n",
       "        0.18847656, -0.07666016, -0.12255859,  0.01428223, -0.10205078,\n",
       "        0.06445312,  0.06079102,  0.04614258,  0.09716797, -0.06835938,\n",
       "       -0.04272461, -0.09423828,  0.00463867, -0.19335938, -0.0057373 ,\n",
       "       -0.04638672,  0.14257812, -0.11279297,  0.05737305, -0.00265503,\n",
       "        0.3046875 , -0.01916504,  0.09716797, -0.09375   ,  0.01574707,\n",
       "        0.00897217,  0.16113281, -0.12988281, -0.08837891,  0.25390625,\n",
       "       -0.04711914, -0.08691406, -0.08398438, -0.27734375, -0.02001953,\n",
       "        0.18066406, -0.06787109,  0.15917969, -0.01196289,  0.15820312,\n",
       "       -0.27734375,  0.0703125 ,  0.03710938, -0.20507812,  0.07958984,\n",
       "       -0.00479126,  0.31835938, -0.05175781, -0.02062988,  0.03320312,\n",
       "        0.125     ,  0.02502441, -0.15332031,  0.06982422, -0.07324219],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv['play']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0208562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating distance between two vectors\n",
    "def distance(p1,p2):\n",
    "    return np.sum((p1-p2)**2)\n",
    "\n",
    "\n",
    "# K-nearest-neighbours algorithm to find the top K neighbours to a word\n",
    "def knn(vector, trained = \"pretrained\", k = 3):\n",
    "    d = []\n",
    "    if trained == \"pretrained\":\n",
    "        for key in wv.index_to_key:\n",
    "\n",
    "            dist = distance(vector,wv[key])\n",
    "            d.append( (dist,key) )\n",
    "\n",
    "        d = np.array(sorted(d))[:,1]\n",
    "        d = d[:k]\n",
    "    else:\n",
    "        for key in emb_model.wv.index_to_key:\n",
    "\n",
    "            dist = distance(vector,emb_model.wv[key])\n",
    "            d.append( (dist,key) )\n",
    "\n",
    "        d = np.array(sorted(d))[:,1]\n",
    "        d = d[:k]\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabd8d10",
   "metadata": {},
   "source": [
    "###  checking semantic similarities\n",
    "1. big − bigger + warmer = warmer, **warm**, chilly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe987bd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['warmer', 'warm', 'chilly'], dtype='<U98')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  big − bigger + warmer\n",
    "semantic_meaning_1 = wv['big'] - wv['bigger'] + wv['warmer']\n",
    "knn(semantic_meaning_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56356224",
   "metadata": {},
   "source": [
    "2. awesome = **amazing**, **fantastic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fceac9ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['awesome', 'amazing', 'fantastic'], dtype='<U98')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# awesome\n",
    "semantic_meaning_2 = wv['awesome']\n",
    "knn(semantic_meaning_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40363846",
   "metadata": {},
   "source": [
    "3. book = **books**, **Booklocker.com**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a8103ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['book', 'books', 'Booklocker.com'], dtype='<U98')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# book  \n",
    "semantic_meaning_3 = wv['book']\n",
    "knn(semantic_meaning_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8be47f4",
   "metadata": {},
   "source": [
    "## (b) Train a Word2Vec model using your own dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44957e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data for training word2vec model —> list of lists of tokens\n",
    "sentences = []\n",
    "for sent in df[\"review_body\"]:\n",
    "    sentences.append(sent.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5dc5baad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.models\n",
    "# defining model\n",
    "emb_model = gensim.models.Word2Vec(sentences=sentences, vector_size = 300, min_count = 9, window = 13)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e752808d",
   "metadata": {},
   "source": [
    "### Semantic similarities using my model\n",
    "1.  big − bigger + warmer = big, huge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "66b05b82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['big', 'huge', 'sometimes'], dtype='<U32')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semantic_meaning_1_mymodel = emb_model.wv['big'] - emb_model.wv['bigger'] + emb_model.wv['warmer']\n",
    "knn(semantic_meaning_1_mymodel, trained=\"trained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3734ff",
   "metadata": {},
   "source": [
    "2. awesome = **fabulous** , **fantastic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a7f0513",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['awesome', 'fabulous', 'fantastic'], dtype='<U32')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semantic_meaning_2_mymodel = emb_model.wv['awesome']\n",
    "knn(semantic_meaning_2_mymodel, trained = \"trained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa42d70",
   "metadata": {},
   "source": [
    "3. book = posts , detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d2d25f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['book', 'posts', 'detail'], dtype='<U32')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semantic_meaning_3_mymodel = emb_model.wv['book']\n",
    "knn(semantic_meaning_3_mymodel, trained = \"trained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2524f8",
   "metadata": {},
   "source": [
    "### `Q — What do you conclude from comparing vectors generated by yourself and the pretrained model? Which of the Word2Vec models seems to encode semantic similarities between words better?`\n",
    "\n",
    "**Ans —** From my observation I conclude that vectors from the pretrained model performed better than vectors trained by me. \n",
    "\n",
    "- Pretrained model gave better relevant results for the queries 1 and 3:— \n",
    "\n",
    "|    | Query                    | Pre-trained model | my model          |\n",
    "|----|--------------------------|-------------------|-------------------|\n",
    "| 1. | big - bigger + warmer =  | warm              | big/huge          |\n",
    "| 2. | awesome =                | amazing/fantastic | fabulous/fantastic |\n",
    "| 3. | book =                   | books, Booklocker.com  | posts/detail            |\n",
    "\n",
    "- The pretrained model performed better on equations because I think it has a better understanding of word meanings. as it was trained on much larger corpora\n",
    "- For simple words like awesome, both models performed well"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05246047",
   "metadata": {},
   "source": [
    "# 3. Simple models (20 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a53096c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Train/Test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['review_body'], df['star_rating'], test_size=0.20, random_state=20)\n",
    "\n",
    "X_train = X_train.values\n",
    "X_test = X_test.values\n",
    "y_train = y_train.values\n",
    "y_test = y_test.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b9e633",
   "metadata": {},
   "source": [
    "#### TF-IDF Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d85e15e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer    \n",
    "# using sk-learns TfidfVectorizer to generate tf-idf vectors\n",
    "vectorizer = TfidfVectorizer(ngram_range = (1,2), min_df = 5,max_features = 10000)\n",
    "\n",
    "X_tfidf_train = vectorizer.fit_transform(X_train)\n",
    "X_tfidf_test = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338d9402",
   "metadata": {},
   "source": [
    "#### Word2vec Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3be899e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_word2v_train = []\n",
    "X_word2v_test = []\n",
    "\n",
    "# converting each training review to a word2vec average vector\n",
    "for review in X_train:\n",
    "    n = len(review)\n",
    "    words = review.split(\" \")\n",
    "    \n",
    "    avg_vector = np.zeros(300,)\n",
    "    \n",
    "    for word in words:\n",
    "        try:\n",
    "            avg_vector += wv[word]\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    avg_vector /= n\n",
    "    X_word2v_train.append(avg_vector)\n",
    "    \n",
    "# converting each test review to a word2vec average vector\n",
    "for review in X_test:\n",
    "\n",
    "    n = len(review)\n",
    "    words = review.split(\" \")\n",
    "    \n",
    "    avg_vector = np.zeros(300,)\n",
    "    \n",
    "    for word in words:\n",
    "        try:\n",
    "            avg_vector += wv[word]\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    avg_vector /= n\n",
    "    X_word2v_test.append(avg_vector)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29b392d",
   "metadata": {},
   "source": [
    "## Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "55c86a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing perceptron and classification_report\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import classification_report\n",
    "#class names for \n",
    "target_names = ['class 1', 'class 2', 'class 3']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdeeeb7f",
   "metadata": {},
   "source": [
    "### Perceptron TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1c227d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Perceptron model TF_IDF\n",
      "\n",
      "Accuracy =  0.6678333333333333\n"
     ]
    }
   ],
   "source": [
    "perceptron = Perceptron(\n",
    "    penalty= 'l1',      # regularization term\n",
    "    alpha=0.000003,     # constant to attached to regularization term\n",
    "    shuffle=True,    \n",
    "    tol=1e-4,           # stopping criteria\n",
    ")\n",
    "perceptron.fit(X_tfidf_train , y_train)\n",
    "\n",
    "y_test_pred_perceptron = perceptron.predict(X_tfidf_test)\n",
    "\n",
    "\n",
    "print( \"Results for Perceptron model TF_IDF\")\n",
    "print()\n",
    "print(\"Accuracy = \", classification_report(y_test, y_test_pred_perceptron, output_dict = True,target_names=target_names)[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf53e0d",
   "metadata": {},
   "source": [
    "### Perceptron Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eb428ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Perceptron model Word2Vec\n",
      "\n",
      "Accuracy =  0.4375\n"
     ]
    }
   ],
   "source": [
    "perceptron = Perceptron(\n",
    "    penalty= 'l1',      # regularization term\n",
    "    alpha=0.000003,     # constant to attached to regularization term\n",
    "    shuffle=True,    \n",
    "    tol=1e-4,           # stopping criteria\n",
    ")\n",
    "perceptron.fit(X_word2v_train , y_train)\n",
    "\n",
    "y_test_pred_perceptron = perceptron.predict(X_word2v_test)\n",
    "\n",
    "print( \"Results for Perceptron model Word2Vec\")\n",
    "print()\n",
    "print(\"Accuracy = \", classification_report(y_test, y_test_pred_perceptron, output_dict = True,target_names=target_names)[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ec9ff7",
   "metadata": {},
   "source": [
    "### Perceptron\n",
    "- TF-IDF Accuracy = 0.667833\n",
    "- Word2vec Accuracy = 0.4375"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06892d94",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6c476114",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "952ade54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for SVM model TF-IDF\n",
      "\n",
      "Accuracy =  0.73725\n"
     ]
    }
   ],
   "source": [
    "svm_classifier = LinearSVC(\n",
    "    penalty='l1',                  # Using L2-Norm\n",
    "    max_iter=2500,\n",
    "    loss='squared_hinge',          # Loss function\n",
    "    dual=False,                    # since n_samples > n_features, preferred to set dual = False\n",
    "    tol=1e-4,              \n",
    "    C=0.5,                        # Regularization parameter\n",
    ")\n",
    "\n",
    "svm_classifier.fit(X_tfidf_train , y_train)\n",
    "y_test_pred_svm = svm_classifier.predict(X_tfidf_test)\n",
    "\n",
    "print( \"Results for SVM model TF-IDF\")\n",
    "print()\n",
    "print(\"Accuracy = \", classification_report(y_test, y_test_pred_svm, output_dict=True, target_names=target_names)[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "944cb5c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for SVM model Word2Vec\n",
      "\n",
      "Accuracy =  0.6260833333333333\n"
     ]
    }
   ],
   "source": [
    "# SVM\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "svm_classifier = LinearSVC(\n",
    "    penalty='l1',                  # Using L2-Norm\n",
    "    max_iter=2500,\n",
    "    loss='squared_hinge',          # Loss function\n",
    "    dual=False,                    # since n_samples > n_features, preferred to set dual = False\n",
    "    tol=1e-4,              \n",
    "    C=0.5,                        # Regularization parameter\n",
    ")\n",
    "\n",
    "svm_classifier.fit(X_word2v_train , y_train)\n",
    "y_test_pred_svm = svm_classifier.predict(X_word2v_test)\n",
    "\n",
    "print( \"Results for SVM model Word2Vec\")\n",
    "print()\n",
    "print(\"Accuracy = \", classification_report(y_test, y_test_pred_svm, output_dict=True, target_names=target_names)[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69baa024",
   "metadata": {},
   "source": [
    "### SVM\n",
    "- TF-IDF Accuracy = 0.73725\n",
    "- word2vec Accuracy = 0.62608333"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab5d372",
   "metadata": {},
   "source": [
    "### `Q - What do you conclude from comparing performances for the models trained using the two different feature types`\n",
    "\n",
    "**Ans —**\n",
    "\n",
    "- I conclude that SVM performs better than than the perceptron model, for both tf-idf and word2vec features. The reason could be that SVM's can handle high-dimensional data and are less sensitive to noisy data.\n",
    "- Tf-idf features performed better than word2vec for both models (SVM & perceptron). The reason could be that TF-IDF  works better than word2vec, when the classification task relies more heavily on the frequency of individual words rather than their semantic meaning. Which might be true since ours is a text classification task, presence of some words might help classify a review.\n",
    "- There was a huge jump in accuracy for the word2vec features when trained on SVM model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795ee64a",
   "metadata": {},
   "source": [
    "# 4. Feedforward Neural Networks (25 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a7f9feb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "22b1bc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting y target labels to One hot encoding for training\n",
    "y_train = y_train.reshape(-1,1)\n",
    "y_test = y_test.reshape(-1,1)\n",
    "\n",
    "onehotencoder = OneHotEncoder(sparse = False).fit(y_train)\n",
    "\n",
    "y_train_ohe = onehotencoder.transform(y_train)\n",
    "y_test_ohe = onehotencoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b37fe5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "test_data = []\n",
    "\n",
    "# creating training data of the format [avg_word2vec, one_hot_label]\n",
    "for i in range(len(y_train)):\n",
    "    train_data.append([X_word2v_train[i],y_train_ohe[i]])\n",
    "\n",
    "# creating test data of the format [avg_word2vec, one_hot_label]\n",
    "for i in range(len(y_test)):\n",
    "    test_data.append([X_word2v_test[i],y_test_ohe[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2fc2d0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating train and test data_loaders for pyTorch training\n",
    "train_loader = torch.utils.data.DataLoader( train_data, batch_size = 10000)\n",
    "test_loader = torch.utils.data.DataLoader( test_data, batch_size = 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c9737d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# took inspiration from the notebook : https://www.kaggle.com/mishra1993/pytorch-multi-layer-perceptron-mnist\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# define the NN architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        hidden_1 = 100\n",
    "        hidden_2 = 10\n",
    "        # linear layer (300 -> hidden_1)\n",
    "        self.fc1 = nn.Linear(300, hidden_1)\n",
    "        # linear layer (n_hidden -> hidden_2)\n",
    "        self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
    "        # linear layer (n_hidden -> 3)\n",
    "        self.fc3 = nn.Linear(hidden_2, 3)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # add hidden layer, with relu activation function\n",
    "        x = x.to(torch.float32)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # add hidden layer, with relu activation function\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # add output layer\n",
    "        x = F.softmax(self.fc3(x),dim=1)\n",
    "        return x\n",
    "\n",
    "# initialize the NN\n",
    "mlp = Net()\n",
    "  \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.AdamW(mlp.parameters(), lr=0.007, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "48b4b43e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.099500\n",
      "Epoch: 2 \tTraining Loss: 1.096505\n",
      "Epoch: 3 \tTraining Loss: 1.090415\n",
      "Epoch: 4 \tTraining Loss: 1.078495\n",
      "Epoch: 5 \tTraining Loss: 1.057826\n",
      "Epoch: 6 \tTraining Loss: 1.029468\n",
      "Epoch: 7 \tTraining Loss: 1.001280\n",
      "Epoch: 8 \tTraining Loss: 0.977913\n",
      "Epoch: 9 \tTraining Loss: 0.959913\n",
      "Epoch: 10 \tTraining Loss: 0.946585\n",
      "Epoch: 11 \tTraining Loss: 0.937092\n",
      "Epoch: 12 \tTraining Loss: 0.930173\n",
      "Epoch: 13 \tTraining Loss: 0.924785\n",
      "Epoch: 14 \tTraining Loss: 0.920523\n",
      "Epoch: 15 \tTraining Loss: 0.917049\n",
      "Epoch: 16 \tTraining Loss: 0.914253\n",
      "Epoch: 17 \tTraining Loss: 0.912000\n",
      "Epoch: 18 \tTraining Loss: 0.910093\n",
      "Epoch: 19 \tTraining Loss: 0.908430\n",
      "Epoch: 20 \tTraining Loss: 0.906939\n",
      "Epoch: 21 \tTraining Loss: 0.905790\n",
      "Epoch: 22 \tTraining Loss: 0.905088\n",
      "Epoch: 23 \tTraining Loss: 0.904785\n",
      "Epoch: 24 \tTraining Loss: 0.902876\n",
      "Epoch: 25 \tTraining Loss: 0.900215\n",
      "Epoch: 26 \tTraining Loss: 0.898803\n",
      "Epoch: 27 \tTraining Loss: 0.898073\n",
      "Epoch: 28 \tTraining Loss: 0.897722\n",
      "Epoch: 29 \tTraining Loss: 0.897283\n",
      "Epoch: 30 \tTraining Loss: 0.896436\n",
      "Epoch: 31 \tTraining Loss: 0.895464\n",
      "Epoch: 32 \tTraining Loss: 0.894687\n",
      "Epoch: 33 \tTraining Loss: 0.894125\n",
      "Epoch: 34 \tTraining Loss: 0.893706\n",
      "Epoch: 35 \tTraining Loss: 0.893321\n",
      "Epoch: 36 \tTraining Loss: 0.892928\n",
      "Epoch: 37 \tTraining Loss: 0.892453\n",
      "Epoch: 38 \tTraining Loss: 0.891935\n",
      "Epoch: 39 \tTraining Loss: 0.891403\n",
      "Epoch: 40 \tTraining Loss: 0.890932\n",
      "Epoch: 41 \tTraining Loss: 0.890494\n",
      "Epoch: 42 \tTraining Loss: 0.890112\n",
      "Epoch: 43 \tTraining Loss: 0.889749\n",
      "Epoch: 44 \tTraining Loss: 0.889403\n",
      "Epoch: 45 \tTraining Loss: 0.889045\n",
      "Epoch: 46 \tTraining Loss: 0.888664\n",
      "Epoch: 47 \tTraining Loss: 0.888249\n",
      "Epoch: 48 \tTraining Loss: 0.887822\n",
      "Epoch: 49 \tTraining Loss: 0.887460\n",
      "Epoch: 50 \tTraining Loss: 0.887127\n",
      "Epoch: 51 \tTraining Loss: 0.886826\n",
      "Epoch: 52 \tTraining Loss: 0.886524\n",
      "Epoch: 53 \tTraining Loss: 0.886250\n",
      "Epoch: 54 \tTraining Loss: 0.885970\n",
      "Epoch: 55 \tTraining Loss: 0.885689\n",
      "Epoch: 56 \tTraining Loss: 0.885457\n",
      "Epoch: 57 \tTraining Loss: 0.885217\n",
      "Epoch: 58 \tTraining Loss: 0.884999\n",
      "Epoch: 59 \tTraining Loss: 0.884792\n",
      "Epoch: 60 \tTraining Loss: 0.884572\n",
      "Epoch: 61 \tTraining Loss: 0.884378\n",
      "Epoch: 62 \tTraining Loss: 0.884186\n",
      "Epoch: 63 \tTraining Loss: 0.884014\n",
      "Epoch: 64 \tTraining Loss: 0.883872\n",
      "Epoch: 65 \tTraining Loss: 0.883734\n",
      "Epoch: 66 \tTraining Loss: 0.883603\n",
      "Epoch: 67 \tTraining Loss: 0.883488\n",
      "Epoch: 68 \tTraining Loss: 0.883381\n",
      "Epoch: 69 \tTraining Loss: 0.883289\n",
      "Epoch: 70 \tTraining Loss: 0.883180\n",
      "Epoch: 71 \tTraining Loss: 0.882992\n",
      "Epoch: 72 \tTraining Loss: 0.882800\n",
      "Epoch: 73 \tTraining Loss: 0.882631\n",
      "Epoch: 74 \tTraining Loss: 0.882632\n",
      "Epoch: 75 \tTraining Loss: 0.882867\n",
      "Epoch: 76 \tTraining Loss: 0.883062\n",
      "Epoch: 77 \tTraining Loss: 0.882684\n",
      "Epoch: 78 \tTraining Loss: 0.881747\n",
      "Epoch: 79 \tTraining Loss: 0.880787\n",
      "Epoch: 80 \tTraining Loss: 0.880132\n",
      "Epoch: 81 \tTraining Loss: 0.879773\n",
      "Epoch: 82 \tTraining Loss: 0.879648\n",
      "Epoch: 83 \tTraining Loss: 0.879646\n",
      "Epoch: 84 \tTraining Loss: 0.879705\n",
      "Epoch: 85 \tTraining Loss: 0.879759\n",
      "Epoch: 86 \tTraining Loss: 0.879777\n",
      "Epoch: 87 \tTraining Loss: 0.879722\n",
      "Epoch: 88 \tTraining Loss: 0.879592\n",
      "Epoch: 89 \tTraining Loss: 0.879375\n",
      "Epoch: 90 \tTraining Loss: 0.879119\n",
      "Epoch: 91 \tTraining Loss: 0.878855\n",
      "Epoch: 92 \tTraining Loss: 0.878626\n",
      "Epoch: 93 \tTraining Loss: 0.878415\n",
      "Epoch: 94 \tTraining Loss: 0.878249\n",
      "Epoch: 95 \tTraining Loss: 0.878119\n",
      "Epoch: 96 \tTraining Loss: 0.877998\n",
      "Epoch: 97 \tTraining Loss: 0.877900\n",
      "Epoch: 98 \tTraining Loss: 0.877811\n",
      "Epoch: 99 \tTraining Loss: 0.877698\n",
      "Epoch: 100 \tTraining Loss: 0.877609\n",
      "Epoch: 101 \tTraining Loss: 0.877511\n",
      "Epoch: 102 \tTraining Loss: 0.877425\n",
      "Epoch: 103 \tTraining Loss: 0.877330\n",
      "Epoch: 104 \tTraining Loss: 0.877260\n",
      "Epoch: 105 \tTraining Loss: 0.877178\n",
      "Epoch: 106 \tTraining Loss: 0.877107\n",
      "Epoch: 107 \tTraining Loss: 0.877013\n",
      "Epoch: 108 \tTraining Loss: 0.876912\n",
      "Epoch: 109 \tTraining Loss: 0.876812\n",
      "Epoch: 110 \tTraining Loss: 0.876701\n",
      "Epoch: 111 \tTraining Loss: 0.876555\n",
      "Epoch: 112 \tTraining Loss: 0.876454\n",
      "Epoch: 113 \tTraining Loss: 0.876304\n",
      "Epoch: 114 \tTraining Loss: 0.876169\n",
      "Epoch: 115 \tTraining Loss: 0.876003\n",
      "Epoch: 116 \tTraining Loss: 0.875865\n",
      "Epoch: 117 \tTraining Loss: 0.875675\n",
      "Epoch: 118 \tTraining Loss: 0.875544\n",
      "Epoch: 119 \tTraining Loss: 0.875385\n",
      "Epoch: 120 \tTraining Loss: 0.875241\n",
      "Epoch: 121 \tTraining Loss: 0.875074\n",
      "Epoch: 122 \tTraining Loss: 0.874960\n",
      "Epoch: 123 \tTraining Loss: 0.874808\n",
      "Epoch: 124 \tTraining Loss: 0.874693\n",
      "Epoch: 125 \tTraining Loss: 0.874544\n",
      "Epoch: 126 \tTraining Loss: 0.874475\n",
      "Epoch: 127 \tTraining Loss: 0.874320\n",
      "Epoch: 128 \tTraining Loss: 0.874251\n",
      "Epoch: 129 \tTraining Loss: 0.874107\n",
      "Epoch: 130 \tTraining Loss: 0.874043\n",
      "Epoch: 131 \tTraining Loss: 0.873895\n",
      "Epoch: 132 \tTraining Loss: 0.873831\n",
      "Epoch: 133 \tTraining Loss: 0.873705\n",
      "Epoch: 134 \tTraining Loss: 0.873633\n",
      "Epoch: 135 \tTraining Loss: 0.873513\n",
      "Epoch: 136 \tTraining Loss: 0.873421\n",
      "Epoch: 137 \tTraining Loss: 0.873326\n",
      "Epoch: 138 \tTraining Loss: 0.873230\n",
      "Epoch: 139 \tTraining Loss: 0.873118\n",
      "Epoch: 140 \tTraining Loss: 0.873018\n",
      "Epoch: 141 \tTraining Loss: 0.872919\n",
      "Epoch: 142 \tTraining Loss: 0.872814\n",
      "Epoch: 143 \tTraining Loss: 0.872679\n",
      "Epoch: 144 \tTraining Loss: 0.872602\n",
      "Epoch: 145 \tTraining Loss: 0.872477\n",
      "Epoch: 146 \tTraining Loss: 0.872416\n",
      "Epoch: 147 \tTraining Loss: 0.872265\n",
      "Epoch: 148 \tTraining Loss: 0.872201\n",
      "Epoch: 149 \tTraining Loss: 0.872056\n",
      "Epoch: 150 \tTraining Loss: 0.871976\n",
      "Epoch: 151 \tTraining Loss: 0.871840\n",
      "Epoch: 152 \tTraining Loss: 0.871751\n",
      "Epoch: 153 \tTraining Loss: 0.871621\n",
      "Epoch: 154 \tTraining Loss: 0.871551\n",
      "Epoch: 155 \tTraining Loss: 0.871432\n",
      "Epoch: 156 \tTraining Loss: 0.871350\n",
      "Epoch: 157 \tTraining Loss: 0.871216\n",
      "Epoch: 158 \tTraining Loss: 0.871117\n",
      "Epoch: 159 \tTraining Loss: 0.871030\n",
      "Epoch: 160 \tTraining Loss: 0.870937\n",
      "Epoch: 161 \tTraining Loss: 0.870836\n",
      "Epoch: 162 \tTraining Loss: 0.870712\n",
      "Epoch: 163 \tTraining Loss: 0.870647\n",
      "Epoch: 164 \tTraining Loss: 0.870537\n",
      "Epoch: 165 \tTraining Loss: 0.870435\n",
      "Epoch: 166 \tTraining Loss: 0.870351\n",
      "Epoch: 167 \tTraining Loss: 0.870254\n",
      "Epoch: 168 \tTraining Loss: 0.870168\n",
      "Epoch: 169 \tTraining Loss: 0.870058\n",
      "Epoch: 170 \tTraining Loss: 0.869943\n",
      "Epoch: 171 \tTraining Loss: 0.869836\n",
      "Epoch: 172 \tTraining Loss: 0.869738\n",
      "Epoch: 173 \tTraining Loss: 0.869639\n",
      "Epoch: 174 \tTraining Loss: 0.869506\n",
      "Epoch: 175 \tTraining Loss: 0.869366\n",
      "Epoch: 176 \tTraining Loss: 0.869295\n",
      "Epoch: 177 \tTraining Loss: 0.869218\n",
      "Epoch: 178 \tTraining Loss: 0.869103\n",
      "Epoch: 179 \tTraining Loss: 0.868950\n",
      "Epoch: 180 \tTraining Loss: 0.868923\n",
      "Epoch: 181 \tTraining Loss: 0.868774\n",
      "Epoch: 182 \tTraining Loss: 0.868709\n",
      "Epoch: 183 \tTraining Loss: 0.868530\n",
      "Epoch: 184 \tTraining Loss: 0.868539\n",
      "Epoch: 185 \tTraining Loss: 0.868365\n",
      "Epoch: 186 \tTraining Loss: 0.868336\n",
      "Epoch: 187 \tTraining Loss: 0.868258\n",
      "Epoch: 188 \tTraining Loss: 0.868094\n",
      "Epoch: 189 \tTraining Loss: 0.867983\n",
      "Epoch: 190 \tTraining Loss: 0.867903\n",
      "Epoch: 191 \tTraining Loss: 0.867793\n",
      "Epoch: 192 \tTraining Loss: 0.867722\n",
      "Epoch: 193 \tTraining Loss: 0.867669\n",
      "Epoch: 194 \tTraining Loss: 0.867473\n",
      "Epoch: 195 \tTraining Loss: 0.867372\n",
      "Epoch: 196 \tTraining Loss: 0.867241\n",
      "Epoch: 197 \tTraining Loss: 0.867268\n",
      "Epoch: 198 \tTraining Loss: 0.867146\n",
      "Epoch: 199 \tTraining Loss: 0.867327\n",
      "Epoch: 200 \tTraining Loss: 0.867220\n",
      "Epoch: 201 \tTraining Loss: 0.867171\n",
      "Epoch: 202 \tTraining Loss: 0.866945\n",
      "Epoch: 203 \tTraining Loss: 0.867324\n",
      "Epoch: 204 \tTraining Loss: 0.867445\n",
      "Epoch: 205 \tTraining Loss: 0.867277\n",
      "Epoch: 206 \tTraining Loss: 0.867591\n",
      "Epoch: 207 \tTraining Loss: 0.867737\n",
      "Epoch: 208 \tTraining Loss: 0.867490\n",
      "Epoch: 209 \tTraining Loss: 0.867804\n",
      "Epoch: 210 \tTraining Loss: 0.867493\n",
      "Epoch: 211 \tTraining Loss: 0.868427\n",
      "Epoch: 212 \tTraining Loss: 0.869703\n",
      "Epoch: 213 \tTraining Loss: 0.872220\n",
      "Epoch: 214 \tTraining Loss: 0.877015\n",
      "Epoch: 215 \tTraining Loss: 0.878796\n",
      "Epoch: 216 \tTraining Loss: 0.873720\n",
      "Epoch: 217 \tTraining Loss: 0.871020\n",
      "Epoch: 218 \tTraining Loss: 0.866543\n",
      "Epoch: 219 \tTraining Loss: 0.865672\n",
      "Epoch: 220 \tTraining Loss: 0.867354\n",
      "Epoch: 221 \tTraining Loss: 0.868377\n",
      "Epoch: 222 \tTraining Loss: 0.868007\n",
      "Epoch: 223 \tTraining Loss: 0.867118\n",
      "Epoch: 224 \tTraining Loss: 0.866564\n",
      "Epoch: 225 \tTraining Loss: 0.865586\n",
      "Epoch: 226 \tTraining Loss: 0.864809\n",
      "Epoch: 227 \tTraining Loss: 0.864393\n",
      "Epoch: 228 \tTraining Loss: 0.864389\n",
      "Epoch: 229 \tTraining Loss: 0.864572\n",
      "Epoch: 230 \tTraining Loss: 0.864798\n",
      "Epoch: 231 \tTraining Loss: 0.864902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 232 \tTraining Loss: 0.864950\n",
      "Epoch: 233 \tTraining Loss: 0.864811\n",
      "Epoch: 234 \tTraining Loss: 0.864700\n",
      "Epoch: 235 \tTraining Loss: 0.864493\n",
      "Epoch: 236 \tTraining Loss: 0.864334\n",
      "Epoch: 237 \tTraining Loss: 0.864050\n",
      "Epoch: 238 \tTraining Loss: 0.863730\n",
      "Epoch: 239 \tTraining Loss: 0.863348\n",
      "Epoch: 240 \tTraining Loss: 0.863025\n",
      "Epoch: 241 \tTraining Loss: 0.862740\n",
      "Epoch: 242 \tTraining Loss: 0.862498\n",
      "Epoch: 243 \tTraining Loss: 0.862336\n",
      "Epoch: 244 \tTraining Loss: 0.862250\n",
      "Epoch: 245 \tTraining Loss: 0.862221\n",
      "Epoch: 246 \tTraining Loss: 0.862233\n",
      "Epoch: 247 \tTraining Loss: 0.862249\n",
      "Epoch: 248 \tTraining Loss: 0.862269\n",
      "Epoch: 249 \tTraining Loss: 0.862320\n",
      "Epoch: 250 \tTraining Loss: 0.862449\n"
     ]
    }
   ],
   "source": [
    "# took inspiration from the notebook : https://www.kaggle.com/mishra1993/pytorch-multi-layer-perceptron-mnist\n",
    "# number of epochs to train the model\n",
    "n_epochs = 250\n",
    "\n",
    "\n",
    "valid_loss_min = np.Inf # set initial \"min\" to infinity\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    mlp.train() # prep model for training\n",
    "    for data, target in train_loader:\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = mlp(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    \n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n",
    "        epoch+1, \n",
    "        train_loss,\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b306c69",
   "metadata": {},
   "source": [
    "## 4 (a) accuracy on the testing split of MLP using `average Word2Vec vectors` = 0.6375833"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "268926cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of MLP on average word2Vec vectors =  0.6375833333333333\n"
     ]
    }
   ],
   "source": [
    "def predict(model, dataloader):\n",
    "    prediction_list = []\n",
    "    acc = 0\n",
    "    for batch, target in dataloader:\n",
    "        outputs = model(batch)\n",
    "        _, predicted = torch.max(outputs.data, 1) \n",
    "        _, actual = torch.max(target.data,1)\n",
    "\n",
    "        prediction_list.append(predicted.cpu())\n",
    "        acc += predicted.numpy()[0] ==  actual.numpy()[0]\n",
    "    \n",
    "    acc /= len(dataloader)\n",
    "    return acc\n",
    "\n",
    "accuracy_on_test = predict(mlp,test_loader)\n",
    "print(\"Accuracy of MLP on average word2Vec vectors = \", accuracy_on_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7b3920",
   "metadata": {},
   "source": [
    "## 4 (b) concatenating first 10 Word2Vec vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "19adefa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting vocab of all the words in pretrained glove vectors\n",
    "vocabulary = set(wv.index_to_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73925df",
   "metadata": {},
   "source": [
    "### Building dataset of concatenated word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3cb1f7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_concatenated_train = []\n",
    "X_concatenated_test = []\n",
    "\n",
    "# Creating concatenated word vector features for training data \n",
    "for i in range(len(X_train)):\n",
    "\n",
    "    concatenated_vect = np.array([])\n",
    "    sent_length = len(X_train[i].split(\" \"))\n",
    "    zero_vectors_to_add = 0\n",
    "    \n",
    "    if sent_length < 10:\n",
    "        zero_vectors_to_add = 10 - sent_length\n",
    "    \n",
    "\n",
    "    # if sent_len >= 10 then just add all words in concatenated vectors\n",
    "    if zero_vectors_to_add == 0:\n",
    "        for idx,word in enumerate(X_train[i].split(\" \")):\n",
    "            if idx == 10:\n",
    "                break\n",
    "                \n",
    "            if word in vocabulary:\n",
    "                concatenated_vect = np.concatenate((concatenated_vect,wv[word]))\n",
    "            else:\n",
    "                concatenated_vect = np.concatenate((concatenated_vect,np.zeros(300,)))\n",
    "\n",
    "    # add as many words as in the review then append by zeros\n",
    "    else:\n",
    "        for word in X_train[i].split(\" \"):\n",
    "            if word in vocabulary:\n",
    "                concatenated_vect = np.concatenate((concatenated_vect,wv[word]))\n",
    "            else:\n",
    "                concatenated_vect = np.concatenate((concatenated_vect,np.zeros(300,)))\n",
    "        \n",
    "        concatenated_vect = np.concatenate(( concatenated_vect, np.zeros( zero_vectors_to_add*300, ) ))\n",
    "    \n",
    "    X_concatenated_train.append(concatenated_vect)\n",
    "    \n",
    "\n",
    "# Creating concatenated word vector features for test data\n",
    "for i in range(len(X_test)):\n",
    "\n",
    "    concatenated_vect = np.array([])\n",
    "    sent_length = len(X_test[i].split(\" \"))\n",
    "    zero_vectors_to_add = 0\n",
    "    \n",
    "    if sent_length < 10:\n",
    "        zero_vectors_to_add = 10 - sent_length\n",
    "    \n",
    "    # if sent_len >= 10 then just add all words in concatenated vectors\n",
    "    if zero_vectors_to_add == 0:\n",
    "        for idx,word in enumerate(X_test[i].split(\" \")):\n",
    "            if idx == 10:\n",
    "                break\n",
    "            if word in vocabulary:\n",
    "                concatenated_vect = np.concatenate((concatenated_vect,wv[word]))\n",
    "            else:\n",
    "                concatenated_vect = np.concatenate((concatenated_vect,np.zeros(300,)))\n",
    "    \n",
    "    # add as many words as in the review then append by zeros\n",
    "    else:\n",
    "        for word in X_test[i].split(\" \"):\n",
    "            if word in vocabulary:\n",
    "                concatenated_vect = np.concatenate((concatenated_vect,wv[word]))\n",
    "            else:\n",
    "                concatenated_vect = np.concatenate((concatenated_vect,np.zeros(300,)))\n",
    "        \n",
    "        concatenated_vect = np.concatenate(( concatenated_vect, np.zeros( zero_vectors_to_add*300, ) ))\n",
    "    \n",
    "    X_concatenated_test.append(concatenated_vect)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4fdc81c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "test_data = []\n",
    "\n",
    "# creating training data of the format [concatenated_word2vec, one_hot_label]\n",
    "for i in range(len(y_train)):\n",
    "    train_data.append([X_concatenated_train[i],y_train_ohe[i]])\n",
    "    \n",
    "\n",
    "# creating testing data of the format [concatenated_word2vec, one_hot_label]\n",
    "for i in range(len(y_test)):\n",
    "    test_data.append([X_concatenated_test[i],y_test_ohe[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bdfdedd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating train and test data loaders for PyTorch model training\n",
    "train_loader = torch.utils.data.DataLoader( train_data, batch_size = 10000)\n",
    "test_loader = torch.utils.data.DataLoader( test_data, batch_size = 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1bc264cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# took inspiration from the notebook : https://www.kaggle.com/mishra1993/pytorch-multi-layer-perceptron-mnist\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# define the NN architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Net, self).__init__()        \n",
    "        self.input_dim = input_dim\n",
    "        hidden_1 = 100\n",
    "        hidden_2 = 10\n",
    "        # linear layer (300 -> hidden_1)\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_1)\n",
    "        # linear layer (n_hidden -> hidden_2)\n",
    "        self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
    "        # linear layer (n_hidden -> 3)\n",
    "        self.fc3 = nn.Linear(hidden_2, 3)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # add hidden layer, with relu activation function\n",
    "        x = x.to(torch.float32)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # add hidden layer, with relu activation function\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # add output layer\n",
    "        x = F.softmax(self.fc3(x),dim=1)\n",
    "        return x\n",
    "\n",
    "# initialize the NN\n",
    "mlp2 = Net(3000)\n",
    "  \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.AdamW(mlp2.parameters(), lr=0.007, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fd94d9b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.087985\n",
      "Epoch: 2 \tTraining Loss: 1.030642\n",
      "Epoch: 3 \tTraining Loss: 0.980365\n",
      "Epoch: 4 \tTraining Loss: 0.960201\n",
      "Epoch: 5 \tTraining Loss: 0.944930\n",
      "Epoch: 6 \tTraining Loss: 0.934093\n",
      "Epoch: 7 \tTraining Loss: 0.923911\n",
      "Epoch: 8 \tTraining Loss: 0.915669\n",
      "Epoch: 9 \tTraining Loss: 0.911624\n",
      "Epoch: 10 \tTraining Loss: 0.901316\n",
      "Epoch: 11 \tTraining Loss: 0.909968\n",
      "Epoch: 12 \tTraining Loss: 0.917659\n",
      "Epoch: 13 \tTraining Loss: 0.903379\n",
      "Epoch: 14 \tTraining Loss: 0.884749\n",
      "Epoch: 15 \tTraining Loss: 0.892234\n",
      "Epoch: 16 \tTraining Loss: 0.873976\n",
      "Epoch: 17 \tTraining Loss: 0.870357\n",
      "Epoch: 18 \tTraining Loss: 0.877344\n",
      "Epoch: 19 \tTraining Loss: 0.901325\n",
      "Epoch: 20 \tTraining Loss: 0.892001\n",
      "Epoch: 21 \tTraining Loss: 0.864076\n",
      "Epoch: 22 \tTraining Loss: 0.860680\n",
      "Epoch: 23 \tTraining Loss: 0.857964\n",
      "Epoch: 24 \tTraining Loss: 0.844885\n",
      "Epoch: 25 \tTraining Loss: 0.830329\n",
      "Epoch: 26 \tTraining Loss: 0.823035\n",
      "Epoch: 27 \tTraining Loss: 0.820873\n",
      "Epoch: 28 \tTraining Loss: 0.820785\n",
      "Epoch: 29 \tTraining Loss: 0.823511\n",
      "Epoch: 30 \tTraining Loss: 0.825432\n",
      "Epoch: 31 \tTraining Loss: 0.834263\n",
      "Epoch: 32 \tTraining Loss: 0.827219\n",
      "Epoch: 33 \tTraining Loss: 0.818262\n",
      "Epoch: 34 \tTraining Loss: 0.800839\n",
      "Epoch: 35 \tTraining Loss: 0.793519\n",
      "Epoch: 36 \tTraining Loss: 0.786595\n",
      "Epoch: 37 \tTraining Loss: 0.783982\n",
      "Epoch: 38 \tTraining Loss: 0.779559\n",
      "Epoch: 39 \tTraining Loss: 0.776988\n",
      "Epoch: 40 \tTraining Loss: 0.776054\n",
      "Epoch: 41 \tTraining Loss: 0.782021\n",
      "Epoch: 42 \tTraining Loss: 0.789878\n",
      "Epoch: 43 \tTraining Loss: 0.788779\n",
      "Epoch: 44 \tTraining Loss: 0.784890\n",
      "Epoch: 45 \tTraining Loss: 0.774825\n",
      "Epoch: 46 \tTraining Loss: 0.758187\n",
      "Epoch: 47 \tTraining Loss: 0.754644\n",
      "Epoch: 48 \tTraining Loss: 0.750622\n",
      "Epoch: 49 \tTraining Loss: 0.745556\n",
      "Epoch: 50 \tTraining Loss: 0.742658\n",
      "Epoch: 51 \tTraining Loss: 0.738902\n",
      "Epoch: 52 \tTraining Loss: 0.736437\n",
      "Epoch: 53 \tTraining Loss: 0.733982\n",
      "Epoch: 54 \tTraining Loss: 0.732444\n",
      "Epoch: 55 \tTraining Loss: 0.731752\n",
      "Epoch: 56 \tTraining Loss: 0.731976\n",
      "Epoch: 57 \tTraining Loss: 0.733137\n",
      "Epoch: 58 \tTraining Loss: 0.734661\n",
      "Epoch: 59 \tTraining Loss: 0.739401\n",
      "Epoch: 60 \tTraining Loss: 0.758313\n",
      "Epoch: 61 \tTraining Loss: 0.758303\n",
      "Epoch: 62 \tTraining Loss: 0.744805\n",
      "Epoch: 63 \tTraining Loss: 0.736602\n",
      "Epoch: 64 \tTraining Loss: 0.730686\n",
      "Epoch: 65 \tTraining Loss: 0.726296\n",
      "Epoch: 66 \tTraining Loss: 0.722052\n",
      "Epoch: 67 \tTraining Loss: 0.721989\n",
      "Epoch: 68 \tTraining Loss: 0.718155\n",
      "Epoch: 69 \tTraining Loss: 0.715754\n",
      "Epoch: 70 \tTraining Loss: 0.712496\n",
      "Epoch: 71 \tTraining Loss: 0.711298\n",
      "Epoch: 72 \tTraining Loss: 0.709851\n",
      "Epoch: 73 \tTraining Loss: 0.708287\n",
      "Epoch: 74 \tTraining Loss: 0.707275\n",
      "Epoch: 75 \tTraining Loss: 0.706474\n",
      "Epoch: 76 \tTraining Loss: 0.705591\n",
      "Epoch: 77 \tTraining Loss: 0.704923\n",
      "Epoch: 78 \tTraining Loss: 0.704227\n",
      "Epoch: 79 \tTraining Loss: 0.703661\n",
      "Epoch: 80 \tTraining Loss: 0.703003\n",
      "Epoch: 81 \tTraining Loss: 0.702522\n",
      "Epoch: 82 \tTraining Loss: 0.701939\n",
      "Epoch: 83 \tTraining Loss: 0.701571\n",
      "Epoch: 84 \tTraining Loss: 0.701141\n",
      "Epoch: 85 \tTraining Loss: 0.700911\n",
      "Epoch: 86 \tTraining Loss: 0.700783\n",
      "Epoch: 87 \tTraining Loss: 0.700426\n",
      "Epoch: 88 \tTraining Loss: 0.700413\n",
      "Epoch: 89 \tTraining Loss: 0.699824\n",
      "Epoch: 90 \tTraining Loss: 0.699761\n",
      "Epoch: 91 \tTraining Loss: 0.699338\n",
      "Epoch: 92 \tTraining Loss: 0.698972\n",
      "Epoch: 93 \tTraining Loss: 0.698729\n",
      "Epoch: 94 \tTraining Loss: 0.698298\n",
      "Epoch: 95 \tTraining Loss: 0.698478\n",
      "Epoch: 96 \tTraining Loss: 0.697981\n",
      "Epoch: 97 \tTraining Loss: 0.697939\n",
      "Epoch: 98 \tTraining Loss: 0.697471\n",
      "Epoch: 99 \tTraining Loss: 0.697366\n",
      "Epoch: 100 \tTraining Loss: 0.697024\n",
      "Epoch: 101 \tTraining Loss: 0.696801\n",
      "Epoch: 102 \tTraining Loss: 0.696556\n",
      "Epoch: 103 \tTraining Loss: 0.696428\n",
      "Epoch: 104 \tTraining Loss: 0.696478\n",
      "Epoch: 105 \tTraining Loss: 0.696145\n",
      "Epoch: 106 \tTraining Loss: 0.696019\n",
      "Epoch: 107 \tTraining Loss: 0.695679\n",
      "Epoch: 108 \tTraining Loss: 0.695639\n",
      "Epoch: 109 \tTraining Loss: 0.695235\n",
      "Epoch: 110 \tTraining Loss: 0.695160\n",
      "Epoch: 111 \tTraining Loss: 0.694820\n",
      "Epoch: 112 \tTraining Loss: 0.694863\n",
      "Epoch: 113 \tTraining Loss: 0.694544\n",
      "Epoch: 114 \tTraining Loss: 0.694504\n",
      "Epoch: 115 \tTraining Loss: 0.694145\n",
      "Epoch: 116 \tTraining Loss: 0.694046\n",
      "Epoch: 117 \tTraining Loss: 0.693967\n",
      "Epoch: 118 \tTraining Loss: 0.693731\n",
      "Epoch: 119 \tTraining Loss: 0.693585\n",
      "Epoch: 120 \tTraining Loss: 0.693427\n",
      "Epoch: 121 \tTraining Loss: 0.693525\n",
      "Epoch: 122 \tTraining Loss: 0.693141\n",
      "Epoch: 123 \tTraining Loss: 0.693048\n",
      "Epoch: 124 \tTraining Loss: 0.692813\n",
      "Epoch: 125 \tTraining Loss: 0.692872\n",
      "Epoch: 126 \tTraining Loss: 0.692633\n",
      "Epoch: 127 \tTraining Loss: 0.692597\n",
      "Epoch: 128 \tTraining Loss: 0.692180\n",
      "Epoch: 129 \tTraining Loss: 0.692344\n",
      "Epoch: 130 \tTraining Loss: 0.692006\n",
      "Epoch: 131 \tTraining Loss: 0.692153\n",
      "Epoch: 132 \tTraining Loss: 0.691717\n",
      "Epoch: 133 \tTraining Loss: 0.691826\n",
      "Epoch: 134 \tTraining Loss: 0.691686\n",
      "Epoch: 135 \tTraining Loss: 0.691619\n",
      "Epoch: 136 \tTraining Loss: 0.691497\n",
      "Epoch: 137 \tTraining Loss: 0.691294\n",
      "Epoch: 138 \tTraining Loss: 0.691383\n",
      "Epoch: 139 \tTraining Loss: 0.691096\n",
      "Epoch: 140 \tTraining Loss: 0.690936\n",
      "Epoch: 141 \tTraining Loss: 0.690697\n",
      "Epoch: 142 \tTraining Loss: 0.690849\n",
      "Epoch: 143 \tTraining Loss: 0.690629\n",
      "Epoch: 144 \tTraining Loss: 0.690627\n",
      "Epoch: 145 \tTraining Loss: 0.690323\n",
      "Epoch: 146 \tTraining Loss: 0.690469\n",
      "Epoch: 147 \tTraining Loss: 0.690250\n",
      "Epoch: 148 \tTraining Loss: 0.690328\n",
      "Epoch: 149 \tTraining Loss: 0.689986\n",
      "Epoch: 150 \tTraining Loss: 0.690170\n",
      "Epoch: 151 \tTraining Loss: 0.689898\n",
      "Epoch: 152 \tTraining Loss: 0.690031\n",
      "Epoch: 153 \tTraining Loss: 0.689603\n",
      "Epoch: 154 \tTraining Loss: 0.689710\n",
      "Epoch: 155 \tTraining Loss: 0.689394\n",
      "Epoch: 156 \tTraining Loss: 0.689589\n",
      "Epoch: 157 \tTraining Loss: 0.689233\n",
      "Epoch: 158 \tTraining Loss: 0.689483\n",
      "Epoch: 159 \tTraining Loss: 0.689225\n",
      "Epoch: 160 \tTraining Loss: 0.689463\n",
      "Epoch: 161 \tTraining Loss: 0.689028\n",
      "Epoch: 162 \tTraining Loss: 0.689084\n",
      "Epoch: 163 \tTraining Loss: 0.689064\n",
      "Epoch: 164 \tTraining Loss: 0.689072\n",
      "Epoch: 165 \tTraining Loss: 0.688757\n",
      "Epoch: 166 \tTraining Loss: 0.688720\n",
      "Epoch: 167 \tTraining Loss: 0.688732\n",
      "Epoch: 168 \tTraining Loss: 0.688696\n",
      "Epoch: 169 \tTraining Loss: 0.688238\n",
      "Epoch: 170 \tTraining Loss: 0.688248\n",
      "Epoch: 171 \tTraining Loss: 0.688220\n",
      "Epoch: 172 \tTraining Loss: 0.688326\n",
      "Epoch: 173 \tTraining Loss: 0.687966\n",
      "Epoch: 174 \tTraining Loss: 0.687708\n",
      "Epoch: 175 \tTraining Loss: 0.687846\n",
      "Epoch: 176 \tTraining Loss: 0.687621\n",
      "Epoch: 177 \tTraining Loss: 0.687698\n",
      "Epoch: 178 \tTraining Loss: 0.687360\n",
      "Epoch: 179 \tTraining Loss: 0.687515\n",
      "Epoch: 180 \tTraining Loss: 0.687225\n",
      "Epoch: 181 \tTraining Loss: 0.687471\n",
      "Epoch: 182 \tTraining Loss: 0.687109\n",
      "Epoch: 183 \tTraining Loss: 0.687273\n",
      "Epoch: 184 \tTraining Loss: 0.687003\n",
      "Epoch: 185 \tTraining Loss: 0.687075\n",
      "Epoch: 186 \tTraining Loss: 0.686746\n",
      "Epoch: 187 \tTraining Loss: 0.686949\n",
      "Epoch: 188 \tTraining Loss: 0.686688\n",
      "Epoch: 189 \tTraining Loss: 0.687036\n",
      "Epoch: 190 \tTraining Loss: 0.686834\n",
      "Epoch: 191 \tTraining Loss: 0.686901\n",
      "Epoch: 192 \tTraining Loss: 0.686562\n",
      "Epoch: 193 \tTraining Loss: 0.686712\n",
      "Epoch: 194 \tTraining Loss: 0.686421\n",
      "Epoch: 195 \tTraining Loss: 0.686487\n",
      "Epoch: 196 \tTraining Loss: 0.686486\n",
      "Epoch: 197 \tTraining Loss: 0.686494\n",
      "Epoch: 198 \tTraining Loss: 0.686264\n",
      "Epoch: 199 \tTraining Loss: 0.686057\n",
      "Epoch: 200 \tTraining Loss: 0.686120\n"
     ]
    }
   ],
   "source": [
    "# took inspiration from the notebook : https://www.kaggle.com/mishra1993/pytorch-multi-layer-perceptron-mnist\n",
    "n_epochs = 200\n",
    "\n",
    "# initialize tracker for minimum validation loss\n",
    "valid_loss_min = np.Inf # set initial \"min\" to infinity\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    mlp2.train() # prep model for training\n",
    "    for data, target in train_loader:\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = mlp2(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    \n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n",
    "        epoch+1, \n",
    "        train_loss,\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c920a276",
   "metadata": {},
   "source": [
    "## 4 (b) accuracy on the testing split of MLP using `concatenated Word2Vec features` =  0.538"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ae9b6d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of MLP on concatenated word2vec features =  0.538\n"
     ]
    }
   ],
   "source": [
    "accuracy_on_test = predict(mlp2,test_loader)\n",
    "print(\"Accuracy of MLP on concatenated word2vec features = \",accuracy_on_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a634a0",
   "metadata": {},
   "source": [
    "### `What do you conclude by comparing accuracy values you obtain with those obtained in the “’Simple Models” section.`\n",
    "**Ans —**\n",
    "- The simple models trained using TF-IDF features performed better than MLP models trained on word2vec\n",
    "- The MLP models performed better than simple models when only using word2vec features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0798dc8",
   "metadata": {},
   "source": [
    "# 5. Recurrent Neural Networks (30 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "69edcc58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-46-5d4041a9d35b>:42: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:233.)\n",
      "  X_rnn_train.append(torch.tensor(list_of_embs))\n"
     ]
    }
   ],
   "source": [
    "X_rnn_train = []\n",
    "X_rnn_test = []\n",
    "\n",
    "# preparing data for rnn-training\n",
    "# of the form [ [emb_w1] , [emb_w2] , [emb_w3] ... ]\n",
    "# shape = (1, 20, 300)\n",
    "for i in range(len(X_train)):\n",
    "    \n",
    "    list_of_embs = []\n",
    "    sent = X_train[i]\n",
    "    words = sent.split(\" \")\n",
    "    \n",
    "    len_of_rev = len(words)\n",
    "    \n",
    "    if len_of_rev >= 20:\n",
    "\n",
    "        for idx,word in enumerate(words):\n",
    "            \n",
    "            if idx == 20:\n",
    "                break\n",
    "\n",
    "            if word in vocabulary:\n",
    "                list_of_embs.append(wv[word])\n",
    "            else:\n",
    "                list_of_embs.append(np.zeros(300,))\n",
    "    \n",
    "    else:\n",
    "        zeros_to_add = 20 - len_of_rev\n",
    "        \n",
    "        for word in words:\n",
    "    \n",
    "            if word in vocabulary:\n",
    "                list_of_embs.append(wv[word])\n",
    "            else:\n",
    "                list_of_embs.append(np.zeros(300,))\n",
    "        \n",
    "        while zeros_to_add != 0:\n",
    "            list_of_embs.append(np.zeros(300,))\n",
    "            zeros_to_add -= 1\n",
    "    \n",
    "    \n",
    "    X_rnn_train.append(torch.tensor(list_of_embs))\n",
    "            \n",
    "\n",
    "# preparing data for rnn test data\n",
    "# of the form [ [emb_w1] , [emb_w2] , [emb_w3] ... ]\n",
    "# shape = (1, 20, 300)\n",
    "\n",
    "for i in range(len(X_test)):\n",
    "    \n",
    "    list_of_embs = []\n",
    "    sent = X_test[i]\n",
    "    words = sent.split(\" \")\n",
    "    \n",
    "    len_of_rev = len(words)\n",
    "    \n",
    "    if len_of_rev >= 20:\n",
    "\n",
    "        for idx,word in enumerate(words):\n",
    "            \n",
    "            if idx == 20:\n",
    "                break\n",
    "\n",
    "            if word in vocabulary:\n",
    "                list_of_embs.append(wv[word])\n",
    "            else:\n",
    "                list_of_embs.append(np.zeros(300,))\n",
    "    \n",
    "    else:\n",
    "        zeros_to_add = 20 - len_of_rev\n",
    "        \n",
    "        for word in words:\n",
    "    \n",
    "            if word in vocabulary:\n",
    "                list_of_embs.append(wv[word])\n",
    "            else:\n",
    "                list_of_embs.append(np.zeros(300,))\n",
    "        \n",
    "        while zeros_to_add != 0:\n",
    "            list_of_embs.append(np.zeros(300,))\n",
    "            zeros_to_add -= 1\n",
    "    \n",
    "    \n",
    "    X_rnn_test.append(torch.tensor(list_of_embs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4b5fd152",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "test_data = []\n",
    "\n",
    "# creating training data of the format [ list of 20 embeddings , one_hot_label]\n",
    "for i in range(len(y_train)):\n",
    "    train_data.append([X_rnn_train[i],y_train_ohe[i]])\n",
    "\n",
    "# creating testing data of the format [ list of 20 embeddings , one_hot_label]\n",
    "for i in range(len(y_test)):\n",
    "    test_data.append([X_rnn_test[i],y_test_ohe[i]])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader( train_data, batch_size = 50)\n",
    "test_loader = torch.utils.data.DataLoader( test_data, batch_size = 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "10c2a08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# took inspiration from — https://www.cs.toronto.edu/~lczhang/360/lec/w06/rnn.html\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Create RNN Model\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
    "        super(RNNModel, self).__init__()\n",
    "        \n",
    "        # Number of hidden dimensions\n",
    "        self.hidden_dim = 20\n",
    "        \n",
    "        # Number of hidden layers\n",
    "        self.layer_dim = 1\n",
    "        \n",
    "        # RNN\n",
    "        self.rnn = nn.RNN(input_size = input_dim, hidden_size = hidden_dim, num_layers = layer_dim, batch_first=True, nonlinearity='relu')\n",
    "        \n",
    "        # Readout layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.to(torch.float32)\n",
    "\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = Variable(torch.zeros(self.layer_dim, x.size(0), self.hidden_dim))\n",
    "            \n",
    "        # One time step\n",
    "        out, hn = self.rnn(x, h0)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "# initialize the NN\n",
    "rnn_model = RNNModel(300, 20, 1, 3)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.AdamW(rnn_model.parameters(), lr=0.007, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01)\n",
    "# optimizer = torch.optim.SGD(rnn_model.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f0f916ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.073449\n",
      "Epoch: 2 \tTraining Loss: 0.982466\n",
      "Epoch: 3 \tTraining Loss: 0.924517\n",
      "Epoch: 4 \tTraining Loss: 0.904837\n",
      "Epoch: 5 \tTraining Loss: 0.886622\n",
      "Epoch: 6 \tTraining Loss: 0.904191\n",
      "Epoch: 7 \tTraining Loss: 0.876907\n",
      "Epoch: 8 \tTraining Loss: 0.864370\n",
      "Epoch: 9 \tTraining Loss: 0.857110\n",
      "Epoch: 10 \tTraining Loss: 0.859337\n",
      "Epoch: 11 \tTraining Loss: 0.859743\n",
      "Epoch: 12 \tTraining Loss: 0.856843\n",
      "Epoch: 13 \tTraining Loss: 0.846146\n",
      "Epoch: 14 \tTraining Loss: 0.847738\n",
      "Epoch: 15 \tTraining Loss: 0.843024\n",
      "Epoch: 16 \tTraining Loss: 0.838097\n",
      "Epoch: 17 \tTraining Loss: 0.837586\n",
      "Epoch: 18 \tTraining Loss: 0.840363\n",
      "Epoch: 19 \tTraining Loss: 0.829435\n",
      "Epoch: 20 \tTraining Loss: 0.838169\n",
      "Epoch: 21 \tTraining Loss: 0.834091\n",
      "Epoch: 22 \tTraining Loss: 0.828550\n",
      "Epoch: 23 \tTraining Loss: 0.846776\n",
      "Epoch: 24 \tTraining Loss: 0.835508\n",
      "Epoch: 25 \tTraining Loss: 0.835366\n",
      "Epoch: 26 \tTraining Loss: 0.830530\n",
      "Epoch: 27 \tTraining Loss: 0.827662\n",
      "Epoch: 28 \tTraining Loss: 0.831931\n",
      "Epoch: 29 \tTraining Loss: 0.899367\n",
      "Epoch: 30 \tTraining Loss: 0.873414\n",
      "Epoch: 31 \tTraining Loss: 0.844549\n",
      "Epoch: 32 \tTraining Loss: 0.836821\n",
      "Epoch: 33 \tTraining Loss: 0.837505\n",
      "Epoch: 34 \tTraining Loss: 0.836536\n",
      "Epoch: 35 \tTraining Loss: 0.844568\n",
      "Epoch: 36 \tTraining Loss: 0.834857\n",
      "Epoch: 37 \tTraining Loss: 0.841563\n",
      "Epoch: 38 \tTraining Loss: 0.828803\n",
      "Epoch: 39 \tTraining Loss: 0.874316\n",
      "Epoch: 40 \tTraining Loss: 0.839318\n",
      "Epoch: 41 \tTraining Loss: 0.831992\n",
      "Epoch: 42 \tTraining Loss: 0.828483\n",
      "Epoch: 43 \tTraining Loss: 0.825315\n",
      "Epoch: 44 \tTraining Loss: 0.824254\n",
      "Epoch: 45 \tTraining Loss: 0.830442\n",
      "Epoch: 46 \tTraining Loss: 0.853518\n",
      "Epoch: 47 \tTraining Loss: 0.844381\n",
      "Epoch: 48 \tTraining Loss: 0.835883\n",
      "Epoch: 49 \tTraining Loss: 0.827475\n",
      "Epoch: 50 \tTraining Loss: 0.830773\n",
      "Epoch: 51 \tTraining Loss: 0.839142\n",
      "Epoch: 52 \tTraining Loss: 0.833359\n",
      "Epoch: 53 \tTraining Loss: 0.829022\n",
      "Epoch: 54 \tTraining Loss: 0.821640\n",
      "Epoch: 55 \tTraining Loss: 0.821223\n",
      "Epoch: 56 \tTraining Loss: 0.819316\n",
      "Epoch: 57 \tTraining Loss: 0.836815\n",
      "Epoch: 58 \tTraining Loss: 0.830935\n",
      "Epoch: 59 \tTraining Loss: 0.821426\n",
      "Epoch: 60 \tTraining Loss: 0.835297\n",
      "Epoch: 61 \tTraining Loss: 0.823233\n",
      "Epoch: 62 \tTraining Loss: 0.841502\n",
      "Epoch: 63 \tTraining Loss: 0.831235\n",
      "Epoch: 64 \tTraining Loss: 0.831748\n",
      "Epoch: 65 \tTraining Loss: 0.826035\n",
      "Epoch: 66 \tTraining Loss: 0.823464\n",
      "Epoch: 67 \tTraining Loss: 0.837168\n",
      "Epoch: 68 \tTraining Loss: 0.829423\n",
      "Epoch: 69 \tTraining Loss: 0.819487\n",
      "Epoch: 70 \tTraining Loss: 0.817722\n",
      "Epoch: 71 \tTraining Loss: 0.820204\n",
      "Epoch: 72 \tTraining Loss: 0.820692\n",
      "Epoch: 73 \tTraining Loss: 0.821145\n",
      "Epoch: 74 \tTraining Loss: 0.815776\n",
      "Epoch: 75 \tTraining Loss: 0.826786\n",
      "Epoch: 76 \tTraining Loss: 0.816106\n",
      "Epoch: 77 \tTraining Loss: 0.823948\n",
      "Epoch: 78 \tTraining Loss: 0.820392\n",
      "Epoch: 79 \tTraining Loss: 0.812672\n",
      "Epoch: 80 \tTraining Loss: 0.815626\n",
      "Epoch: 81 \tTraining Loss: 0.812889\n",
      "Epoch: 82 \tTraining Loss: 0.815917\n",
      "Epoch: 83 \tTraining Loss: 0.816354\n",
      "Epoch: 84 \tTraining Loss: 0.824945\n",
      "Epoch: 85 \tTraining Loss: 0.826380\n",
      "Epoch: 86 \tTraining Loss: 0.823774\n",
      "Epoch: 87 \tTraining Loss: 0.814110\n",
      "Epoch: 88 \tTraining Loss: 0.806555\n",
      "Epoch: 89 \tTraining Loss: 0.812431\n",
      "Epoch: 90 \tTraining Loss: 0.813462\n",
      "Epoch: 91 \tTraining Loss: 0.818436\n",
      "Epoch: 92 \tTraining Loss: 0.838663\n",
      "Epoch: 93 \tTraining Loss: 0.824462\n",
      "Epoch: 94 \tTraining Loss: 0.815641\n",
      "Epoch: 95 \tTraining Loss: 0.812385\n",
      "Epoch: 96 \tTraining Loss: 0.809518\n",
      "Epoch: 97 \tTraining Loss: 0.815546\n",
      "Epoch: 98 \tTraining Loss: 0.815147\n",
      "Epoch: 99 \tTraining Loss: 0.817699\n",
      "Epoch: 100 \tTraining Loss: 0.814929\n",
      "Epoch: 101 \tTraining Loss: 0.821102\n",
      "Epoch: 102 \tTraining Loss: 0.829028\n",
      "Epoch: 103 \tTraining Loss: 0.822026\n",
      "Epoch: 104 \tTraining Loss: 0.815934\n",
      "Epoch: 105 \tTraining Loss: 0.818246\n",
      "Epoch: 106 \tTraining Loss: 0.812100\n",
      "Epoch: 107 \tTraining Loss: 0.808245\n",
      "Epoch: 108 \tTraining Loss: 0.812619\n",
      "Epoch: 109 \tTraining Loss: 0.809481\n",
      "Epoch: 110 \tTraining Loss: 0.823557\n",
      "Epoch: 111 \tTraining Loss: 0.815203\n",
      "Epoch: 112 \tTraining Loss: 0.808271\n",
      "Epoch: 113 \tTraining Loss: 0.808490\n",
      "Epoch: 114 \tTraining Loss: 0.816783\n",
      "Epoch: 115 \tTraining Loss: 0.805790\n",
      "Epoch: 116 \tTraining Loss: 0.809002\n",
      "Epoch: 117 \tTraining Loss: 0.807652\n",
      "Epoch: 118 \tTraining Loss: 0.808594\n",
      "Epoch: 119 \tTraining Loss: 0.811466\n",
      "Epoch: 120 \tTraining Loss: 0.808499\n",
      "Epoch: 121 \tTraining Loss: 0.811584\n",
      "Epoch: 122 \tTraining Loss: 0.808953\n",
      "Epoch: 123 \tTraining Loss: 0.807976\n",
      "Epoch: 124 \tTraining Loss: 0.816180\n",
      "Epoch: 125 \tTraining Loss: 0.821739\n",
      "Epoch: 126 \tTraining Loss: 0.809106\n",
      "Epoch: 127 \tTraining Loss: 0.833805\n",
      "Epoch: 128 \tTraining Loss: 0.822196\n",
      "Epoch: 129 \tTraining Loss: 0.809029\n",
      "Epoch: 130 \tTraining Loss: 0.804619\n",
      "Epoch: 131 \tTraining Loss: 0.807534\n",
      "Epoch: 132 \tTraining Loss: 0.815551\n",
      "Epoch: 133 \tTraining Loss: 0.828665\n",
      "Epoch: 134 \tTraining Loss: 0.806686\n",
      "Epoch: 135 \tTraining Loss: 0.814347\n",
      "Epoch: 136 \tTraining Loss: 0.825026\n",
      "Epoch: 137 \tTraining Loss: 0.843226\n",
      "Epoch: 138 \tTraining Loss: 0.818764\n",
      "Epoch: 139 \tTraining Loss: 0.813505\n",
      "Epoch: 140 \tTraining Loss: 0.809138\n",
      "Epoch: 141 \tTraining Loss: 0.813916\n",
      "Epoch: 142 \tTraining Loss: 0.808109\n",
      "Epoch: 143 \tTraining Loss: 0.813825\n",
      "Epoch: 144 \tTraining Loss: 0.814046\n",
      "Epoch: 145 \tTraining Loss: 0.815203\n",
      "Epoch: 146 \tTraining Loss: 0.808346\n",
      "Epoch: 147 \tTraining Loss: 0.807259\n",
      "Epoch: 148 \tTraining Loss: 0.804439\n",
      "Epoch: 149 \tTraining Loss: 0.810955\n",
      "Epoch: 150 \tTraining Loss: 0.815000\n",
      "Epoch: 151 \tTraining Loss: 0.811175\n",
      "Epoch: 152 \tTraining Loss: 0.809021\n",
      "Epoch: 153 \tTraining Loss: 0.807761\n",
      "Epoch: 154 \tTraining Loss: 0.802290\n",
      "Epoch: 155 \tTraining Loss: 0.807203\n",
      "Epoch: 156 \tTraining Loss: 0.817054\n",
      "Epoch: 157 \tTraining Loss: 0.813522\n",
      "Epoch: 158 \tTraining Loss: 0.806994\n",
      "Epoch: 159 \tTraining Loss: 0.802420\n",
      "Epoch: 160 \tTraining Loss: 0.809086\n",
      "Epoch: 161 \tTraining Loss: 0.814154\n",
      "Epoch: 162 \tTraining Loss: 0.825420\n",
      "Epoch: 163 \tTraining Loss: 0.838254\n",
      "Epoch: 164 \tTraining Loss: 0.840964\n",
      "Epoch: 165 \tTraining Loss: 0.815900\n",
      "Epoch: 166 \tTraining Loss: 0.814966\n",
      "Epoch: 167 \tTraining Loss: 0.813060\n",
      "Epoch: 168 \tTraining Loss: 0.803455\n",
      "Epoch: 169 \tTraining Loss: 0.806726\n",
      "Epoch: 170 \tTraining Loss: 0.811421\n",
      "Epoch: 171 \tTraining Loss: 0.805918\n",
      "Epoch: 172 \tTraining Loss: 0.807284\n",
      "Epoch: 173 \tTraining Loss: 0.806944\n",
      "Epoch: 174 \tTraining Loss: 0.802814\n",
      "Epoch: 175 \tTraining Loss: 0.829186\n",
      "Epoch: 176 \tTraining Loss: 0.813969\n",
      "Epoch: 177 \tTraining Loss: 0.813336\n",
      "Epoch: 178 \tTraining Loss: 0.810942\n",
      "Epoch: 179 \tTraining Loss: 0.805262\n",
      "Epoch: 180 \tTraining Loss: 0.810702\n",
      "Epoch: 181 \tTraining Loss: 0.814303\n",
      "Epoch: 182 \tTraining Loss: 0.807249\n",
      "Epoch: 183 \tTraining Loss: 0.799527\n",
      "Epoch: 184 \tTraining Loss: 0.800174\n",
      "Epoch: 185 \tTraining Loss: 0.809624\n",
      "Epoch: 186 \tTraining Loss: 0.802753\n",
      "Epoch: 187 \tTraining Loss: 0.800985\n",
      "Epoch: 188 \tTraining Loss: 0.800650\n",
      "Epoch: 189 \tTraining Loss: 0.803152\n",
      "Epoch: 190 \tTraining Loss: 0.805742\n",
      "Epoch: 191 \tTraining Loss: 0.811693\n",
      "Epoch: 192 \tTraining Loss: 0.810373\n",
      "Epoch: 193 \tTraining Loss: 0.815319\n",
      "Epoch: 194 \tTraining Loss: 0.807186\n",
      "Epoch: 195 \tTraining Loss: 0.804960\n",
      "Epoch: 196 \tTraining Loss: 0.810005\n",
      "Epoch: 197 \tTraining Loss: 0.812347\n",
      "Epoch: 198 \tTraining Loss: 0.803384\n",
      "Epoch: 199 \tTraining Loss: 0.807675\n",
      "Epoch: 200 \tTraining Loss: 0.809771\n"
     ]
    }
   ],
   "source": [
    "# took inspiration from the notebook : https://www.kaggle.com/mishra1993/pytorch-multi-layer-perceptron-mnist\n",
    "n_epochs = 200\n",
    "\n",
    "valid_loss_min = np.Inf # set initial \"min\" to infinity\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    rnn_model.train() # prep model for training\n",
    "    for data, target in train_loader:\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = rnn_model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    \n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n",
    "        epoch+1, \n",
    "        train_loss,\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff10aa6",
   "metadata": {},
   "source": [
    "## Accuracy of RNN mdoel in test set =  0.5745833"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4ef1ef16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of RNN mdoel in test set =  0.5745833333333333\n"
     ]
    }
   ],
   "source": [
    "accuracy_on_test = predict(rnn_model,test_loader)\n",
    "print(\"Accuracy of RNN mdoel in test set = \",accuracy_on_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c089d4",
   "metadata": {},
   "source": [
    "### `Q — What do you conclude by comparing accuracy values you obtain with those obtained with feedforward neural network models`\n",
    "\n",
    "**Ans —**\n",
    "- I do not observe much improvement using an RNN instead of simple Feed Forward Network. RNNs generally perform better than FNNs because they are specifically built for NLP sequence tasks. But since, a text classification problem relies heavily on presence of few words, I think the sequence understanding/order of words did not mater in this case and FNNs performed equally well.\n",
    "\n",
    "- The FNNs got an accuracy of *0.637583333* on avg_word2vec and *0.538* on concatenated_word2vec. While the RNN got an accuracy of 0.5745\n",
    "\n",
    "- The accuracy is better than concatenated vectors but not better than avg_word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b191cc4",
   "metadata": {},
   "source": [
    "## 5 (b) gated recurrent unit cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "222041b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# took inspiration from — https://www.cs.toronto.edu/~lczhang/360/lec/w06/rnn.html\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
    "        super(GRUModel, self).__init__()\n",
    "        \n",
    "        # Number of hidden dimensions\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Number of hidden layers\n",
    "        self.layer_dim = layer_dim\n",
    "        \n",
    "        # RNN\n",
    "        self.gru = nn.GRU(input_size = input_dim, hidden_size = hidden_dim, num_layers = layer_dim, batch_first=True)\n",
    "        \n",
    "        # Readout layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.to(torch.float32)\n",
    "\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = Variable(torch.zeros(self.layer_dim, x.size(0), self.hidden_dim))\n",
    "            \n",
    "        # One time step\n",
    "        out, hn = self.gru(x, h0)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "# initialize the NN\n",
    "gru_model = GRUModel(300, 20, 1, 3)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer = torch.optim.AdamW(rnn_model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01)\n",
    "optimizer = torch.optim.SGD(gru_model.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ad9820be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.100832\n",
      "Epoch: 2 \tTraining Loss: 1.097882\n",
      "Epoch: 3 \tTraining Loss: 1.095954\n",
      "Epoch: 4 \tTraining Loss: 1.094364\n",
      "Epoch: 5 \tTraining Loss: 1.092962\n",
      "Epoch: 6 \tTraining Loss: 1.091654\n",
      "Epoch: 7 \tTraining Loss: 1.090370\n",
      "Epoch: 8 \tTraining Loss: 1.089049\n",
      "Epoch: 9 \tTraining Loss: 1.087631\n",
      "Epoch: 10 \tTraining Loss: 1.086047\n",
      "Epoch: 11 \tTraining Loss: 1.084203\n",
      "Epoch: 12 \tTraining Loss: 1.081935\n",
      "Epoch: 13 \tTraining Loss: 1.078899\n",
      "Epoch: 14 \tTraining Loss: 1.074129\n",
      "Epoch: 15 \tTraining Loss: 1.062670\n",
      "Epoch: 16 \tTraining Loss: 1.007904\n",
      "Epoch: 17 \tTraining Loss: 0.967296\n",
      "Epoch: 18 \tTraining Loss: 0.953046\n",
      "Epoch: 19 \tTraining Loss: 0.944091\n",
      "Epoch: 20 \tTraining Loss: 0.937424\n",
      "Epoch: 21 \tTraining Loss: 0.931976\n",
      "Epoch: 22 \tTraining Loss: 0.927312\n",
      "Epoch: 23 \tTraining Loss: 0.923218\n",
      "Epoch: 24 \tTraining Loss: 0.919553\n",
      "Epoch: 25 \tTraining Loss: 0.916211\n",
      "Epoch: 26 \tTraining Loss: 0.913102\n",
      "Epoch: 27 \tTraining Loss: 0.910153\n",
      "Epoch: 28 \tTraining Loss: 0.907296\n",
      "Epoch: 29 \tTraining Loss: 0.904465\n",
      "Epoch: 30 \tTraining Loss: 0.901583\n",
      "Epoch: 31 \tTraining Loss: 0.898543\n",
      "Epoch: 32 \tTraining Loss: 0.895176\n",
      "Epoch: 33 \tTraining Loss: 0.891209\n",
      "Epoch: 34 \tTraining Loss: 0.886337\n",
      "Epoch: 35 \tTraining Loss: 0.880839\n",
      "Epoch: 36 \tTraining Loss: 0.875914\n",
      "Epoch: 37 \tTraining Loss: 0.871902\n",
      "Epoch: 38 \tTraining Loss: 0.868483\n",
      "Epoch: 39 \tTraining Loss: 0.865471\n",
      "Epoch: 40 \tTraining Loss: 0.862763\n",
      "Epoch: 41 \tTraining Loss: 0.860291\n",
      "Epoch: 42 \tTraining Loss: 0.858007\n",
      "Epoch: 43 \tTraining Loss: 0.855877\n",
      "Epoch: 44 \tTraining Loss: 0.853873\n",
      "Epoch: 45 \tTraining Loss: 0.851975\n",
      "Epoch: 46 \tTraining Loss: 0.850165\n",
      "Epoch: 47 \tTraining Loss: 0.848431\n",
      "Epoch: 48 \tTraining Loss: 0.846763\n",
      "Epoch: 49 \tTraining Loss: 0.845151\n",
      "Epoch: 50 \tTraining Loss: 0.843591\n",
      "Epoch: 51 \tTraining Loss: 0.842075\n",
      "Epoch: 52 \tTraining Loss: 0.840600\n",
      "Epoch: 53 \tTraining Loss: 0.839163\n",
      "Epoch: 54 \tTraining Loss: 0.837759\n",
      "Epoch: 55 \tTraining Loss: 0.836388\n",
      "Epoch: 56 \tTraining Loss: 0.835046\n",
      "Epoch: 57 \tTraining Loss: 0.833731\n",
      "Epoch: 58 \tTraining Loss: 0.832443\n",
      "Epoch: 59 \tTraining Loss: 0.831180\n",
      "Epoch: 60 \tTraining Loss: 0.829941\n",
      "Epoch: 61 \tTraining Loss: 0.828725\n",
      "Epoch: 62 \tTraining Loss: 0.827530\n",
      "Epoch: 63 \tTraining Loss: 0.826356\n",
      "Epoch: 64 \tTraining Loss: 0.825202\n",
      "Epoch: 65 \tTraining Loss: 0.824067\n",
      "Epoch: 66 \tTraining Loss: 0.822951\n",
      "Epoch: 67 \tTraining Loss: 0.821853\n",
      "Epoch: 68 \tTraining Loss: 0.820772\n",
      "Epoch: 69 \tTraining Loss: 0.819708\n",
      "Epoch: 70 \tTraining Loss: 0.818659\n",
      "Epoch: 71 \tTraining Loss: 0.817626\n",
      "Epoch: 72 \tTraining Loss: 0.816607\n",
      "Epoch: 73 \tTraining Loss: 0.815602\n",
      "Epoch: 74 \tTraining Loss: 0.814611\n",
      "Epoch: 75 \tTraining Loss: 0.813633\n",
      "Epoch: 76 \tTraining Loss: 0.812667\n",
      "Epoch: 77 \tTraining Loss: 0.811713\n",
      "Epoch: 78 \tTraining Loss: 0.810770\n",
      "Epoch: 79 \tTraining Loss: 0.809838\n",
      "Epoch: 80 \tTraining Loss: 0.808917\n",
      "Epoch: 81 \tTraining Loss: 0.808005\n",
      "Epoch: 82 \tTraining Loss: 0.807103\n",
      "Epoch: 83 \tTraining Loss: 0.806210\n",
      "Epoch: 84 \tTraining Loss: 0.805326\n",
      "Epoch: 85 \tTraining Loss: 0.804450\n",
      "Epoch: 86 \tTraining Loss: 0.803583\n",
      "Epoch: 87 \tTraining Loss: 0.802723\n",
      "Epoch: 88 \tTraining Loss: 0.801871\n",
      "Epoch: 89 \tTraining Loss: 0.801025\n",
      "Epoch: 90 \tTraining Loss: 0.800187\n",
      "Epoch: 91 \tTraining Loss: 0.799356\n",
      "Epoch: 92 \tTraining Loss: 0.798531\n",
      "Epoch: 93 \tTraining Loss: 0.797712\n",
      "Epoch: 94 \tTraining Loss: 0.796899\n",
      "Epoch: 95 \tTraining Loss: 0.796093\n",
      "Epoch: 96 \tTraining Loss: 0.795292\n",
      "Epoch: 97 \tTraining Loss: 0.794497\n",
      "Epoch: 98 \tTraining Loss: 0.793707\n",
      "Epoch: 99 \tTraining Loss: 0.792923\n",
      "Epoch: 100 \tTraining Loss: 0.792144\n",
      "Epoch: 101 \tTraining Loss: 0.791370\n",
      "Epoch: 102 \tTraining Loss: 0.790601\n",
      "Epoch: 103 \tTraining Loss: 0.789838\n",
      "Epoch: 104 \tTraining Loss: 0.789079\n",
      "Epoch: 105 \tTraining Loss: 0.788326\n",
      "Epoch: 106 \tTraining Loss: 0.787577\n",
      "Epoch: 107 \tTraining Loss: 0.786833\n",
      "Epoch: 108 \tTraining Loss: 0.786093\n",
      "Epoch: 109 \tTraining Loss: 0.785358\n",
      "Epoch: 110 \tTraining Loss: 0.784628\n",
      "Epoch: 111 \tTraining Loss: 0.783903\n",
      "Epoch: 112 \tTraining Loss: 0.783182\n",
      "Epoch: 113 \tTraining Loss: 0.782465\n",
      "Epoch: 114 \tTraining Loss: 0.781753\n",
      "Epoch: 115 \tTraining Loss: 0.781046\n",
      "Epoch: 116 \tTraining Loss: 0.780343\n",
      "Epoch: 117 \tTraining Loss: 0.779644\n",
      "Epoch: 118 \tTraining Loss: 0.778950\n",
      "Epoch: 119 \tTraining Loss: 0.778259\n",
      "Epoch: 120 \tTraining Loss: 0.777574\n",
      "Epoch: 121 \tTraining Loss: 0.776892\n",
      "Epoch: 122 \tTraining Loss: 0.776215\n",
      "Epoch: 123 \tTraining Loss: 0.775541\n",
      "Epoch: 124 \tTraining Loss: 0.774872\n",
      "Epoch: 125 \tTraining Loss: 0.774207\n",
      "Epoch: 126 \tTraining Loss: 0.773546\n",
      "Epoch: 127 \tTraining Loss: 0.772889\n",
      "Epoch: 128 \tTraining Loss: 0.772236\n",
      "Epoch: 129 \tTraining Loss: 0.771587\n",
      "Epoch: 130 \tTraining Loss: 0.770942\n",
      "Epoch: 131 \tTraining Loss: 0.770300\n",
      "Epoch: 132 \tTraining Loss: 0.769663\n",
      "Epoch: 133 \tTraining Loss: 0.769029\n",
      "Epoch: 134 \tTraining Loss: 0.768398\n",
      "Epoch: 135 \tTraining Loss: 0.767772\n",
      "Epoch: 136 \tTraining Loss: 0.767149\n",
      "Epoch: 137 \tTraining Loss: 0.766529\n",
      "Epoch: 138 \tTraining Loss: 0.765913\n",
      "Epoch: 139 \tTraining Loss: 0.765301\n",
      "Epoch: 140 \tTraining Loss: 0.764692\n",
      "Epoch: 141 \tTraining Loss: 0.764086\n",
      "Epoch: 142 \tTraining Loss: 0.763483\n",
      "Epoch: 143 \tTraining Loss: 0.762884\n",
      "Epoch: 144 \tTraining Loss: 0.762288\n",
      "Epoch: 145 \tTraining Loss: 0.761695\n",
      "Epoch: 146 \tTraining Loss: 0.761105\n",
      "Epoch: 147 \tTraining Loss: 0.760518\n",
      "Epoch: 148 \tTraining Loss: 0.759934\n",
      "Epoch: 149 \tTraining Loss: 0.759353\n",
      "Epoch: 150 \tTraining Loss: 0.758775\n",
      "Epoch: 151 \tTraining Loss: 0.758199\n",
      "Epoch: 152 \tTraining Loss: 0.757627\n",
      "Epoch: 153 \tTraining Loss: 0.757057\n",
      "Epoch: 154 \tTraining Loss: 0.756490\n",
      "Epoch: 155 \tTraining Loss: 0.755925\n",
      "Epoch: 156 \tTraining Loss: 0.755363\n",
      "Epoch: 157 \tTraining Loss: 0.754804\n",
      "Epoch: 158 \tTraining Loss: 0.754247\n",
      "Epoch: 159 \tTraining Loss: 0.753693\n",
      "Epoch: 160 \tTraining Loss: 0.753141\n",
      "Epoch: 161 \tTraining Loss: 0.752591\n",
      "Epoch: 162 \tTraining Loss: 0.752044\n",
      "Epoch: 163 \tTraining Loss: 0.751499\n",
      "Epoch: 164 \tTraining Loss: 0.750957\n",
      "Epoch: 165 \tTraining Loss: 0.750416\n",
      "Epoch: 166 \tTraining Loss: 0.749878\n",
      "Epoch: 167 \tTraining Loss: 0.749342\n",
      "Epoch: 168 \tTraining Loss: 0.748809\n",
      "Epoch: 169 \tTraining Loss: 0.748277\n",
      "Epoch: 170 \tTraining Loss: 0.747747\n",
      "Epoch: 171 \tTraining Loss: 0.747220\n",
      "Epoch: 172 \tTraining Loss: 0.746694\n",
      "Epoch: 173 \tTraining Loss: 0.746171\n",
      "Epoch: 174 \tTraining Loss: 0.745649\n",
      "Epoch: 175 \tTraining Loss: 0.745129\n",
      "Epoch: 176 \tTraining Loss: 0.744612\n",
      "Epoch: 177 \tTraining Loss: 0.744096\n",
      "Epoch: 178 \tTraining Loss: 0.743582\n",
      "Epoch: 179 \tTraining Loss: 0.743069\n",
      "Epoch: 180 \tTraining Loss: 0.742559\n",
      "Epoch: 181 \tTraining Loss: 0.742050\n",
      "Epoch: 182 \tTraining Loss: 0.741543\n",
      "Epoch: 183 \tTraining Loss: 0.741038\n",
      "Epoch: 184 \tTraining Loss: 0.740535\n",
      "Epoch: 185 \tTraining Loss: 0.740033\n",
      "Epoch: 186 \tTraining Loss: 0.739533\n",
      "Epoch: 187 \tTraining Loss: 0.739034\n",
      "Epoch: 188 \tTraining Loss: 0.738537\n",
      "Epoch: 189 \tTraining Loss: 0.738042\n",
      "Epoch: 190 \tTraining Loss: 0.737548\n",
      "Epoch: 191 \tTraining Loss: 0.737056\n",
      "Epoch: 192 \tTraining Loss: 0.736565\n",
      "Epoch: 193 \tTraining Loss: 0.736076\n",
      "Epoch: 194 \tTraining Loss: 0.735588\n",
      "Epoch: 195 \tTraining Loss: 0.735101\n",
      "Epoch: 196 \tTraining Loss: 0.734616\n",
      "Epoch: 197 \tTraining Loss: 0.734133\n",
      "Epoch: 198 \tTraining Loss: 0.733650\n",
      "Epoch: 199 \tTraining Loss: 0.733169\n",
      "Epoch: 200 \tTraining Loss: 0.732690\n"
     ]
    }
   ],
   "source": [
    "# took inspiration from the notebook : https://www.kaggle.com/mishra1993/pytorch-multi-layer-perceptron-mnist\n",
    "\n",
    "n_epochs = 200\n",
    "\n",
    "valid_loss_min = np.Inf # set initial \"min\" to infinity\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    gru_model.train() # prep model for training\n",
    "    for data, target in train_loader:\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = gru_model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    \n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n",
    "        epoch+1, \n",
    "        train_loss,\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d130823a",
   "metadata": {},
   "source": [
    "## Accuracy for GRU model is = 0.6311666"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5474ce77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for GRU model is =  0.6311666666666667\n"
     ]
    }
   ],
   "source": [
    "accuracy_on_test = predict(gru_model,test_loader)\n",
    "print(\"Accuracy for GRU model is = \",accuracy_on_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae008446",
   "metadata": {},
   "source": [
    "## 5 (c) LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "16524c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# took inspiration from — https://www.cs.toronto.edu/~lczhang/360/lec/w06/rnn.html\n",
    "class LSTMmodel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
    "        super(LSTMmodel, self).__init__()\n",
    "        \n",
    "        # Number of hidden dimensions\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Number of hidden layers\n",
    "        self.layer_dim = layer_dim\n",
    "        \n",
    "        # RNN\n",
    "        self.lstm = nn.LSTM(input_size = input_dim, hidden_size = hidden_dim, num_layers = layer_dim, batch_first=True)\n",
    "        \n",
    "        # Readout layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.to(torch.float32)\n",
    "\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = Variable(torch.zeros(self.layer_dim, x.size(0), self.hidden_dim))\n",
    "        c0 = Variable(torch.zeros(self.layer_dim, x.size(0), self.hidden_dim))\n",
    "            \n",
    "        # One time step\n",
    "        out, (hn,cn) = self.lstm(x, (h0.detach(),c0.detach()) )   \n",
    "        out = self.fc(out[:, -1, :])\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "# initialize the NN\n",
    "lstm_model = LSTMmodel(300, 20, 1, 3)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer = torch.optim.AdamW(rnn_model.parameters(), lr=0.007, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01)\n",
    "optimizer = torch.optim.SGD(lstm_model.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f6f93d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.100098\n",
      "Epoch: 2 \tTraining Loss: 1.099015\n",
      "Epoch: 3 \tTraining Loss: 1.098353\n",
      "Epoch: 4 \tTraining Loss: 1.097724\n",
      "Epoch: 5 \tTraining Loss: 1.097109\n",
      "Epoch: 6 \tTraining Loss: 1.096490\n",
      "Epoch: 7 \tTraining Loss: 1.095854\n",
      "Epoch: 8 \tTraining Loss: 1.095191\n",
      "Epoch: 9 \tTraining Loss: 1.094492\n",
      "Epoch: 10 \tTraining Loss: 1.093750\n",
      "Epoch: 11 \tTraining Loss: 1.092954\n",
      "Epoch: 12 \tTraining Loss: 1.092094\n",
      "Epoch: 13 \tTraining Loss: 1.091153\n",
      "Epoch: 14 \tTraining Loss: 1.090102\n",
      "Epoch: 15 \tTraining Loss: 1.088891\n",
      "Epoch: 16 \tTraining Loss: 1.087414\n",
      "Epoch: 17 \tTraining Loss: 1.085436\n",
      "Epoch: 18 \tTraining Loss: 1.082238\n",
      "Epoch: 19 \tTraining Loss: 1.073470\n",
      "Epoch: 20 \tTraining Loss: 1.018304\n",
      "Epoch: 21 \tTraining Loss: 0.978978\n",
      "Epoch: 22 \tTraining Loss: 0.963936\n",
      "Epoch: 23 \tTraining Loss: 0.953896\n",
      "Epoch: 24 \tTraining Loss: 0.945958\n",
      "Epoch: 25 \tTraining Loss: 0.939814\n",
      "Epoch: 26 \tTraining Loss: 0.935063\n",
      "Epoch: 27 \tTraining Loss: 0.931097\n",
      "Epoch: 28 \tTraining Loss: 0.927576\n",
      "Epoch: 29 \tTraining Loss: 0.924344\n",
      "Epoch: 30 \tTraining Loss: 0.921309\n",
      "Epoch: 31 \tTraining Loss: 0.918399\n",
      "Epoch: 32 \tTraining Loss: 0.915548\n",
      "Epoch: 33 \tTraining Loss: 0.912673\n",
      "Epoch: 34 \tTraining Loss: 0.909617\n",
      "Epoch: 35 \tTraining Loss: 0.906016\n",
      "Epoch: 36 \tTraining Loss: 0.900981\n",
      "Epoch: 37 \tTraining Loss: 0.894595\n",
      "Epoch: 38 \tTraining Loss: 0.889412\n",
      "Epoch: 39 \tTraining Loss: 0.885380\n",
      "Epoch: 40 \tTraining Loss: 0.882007\n",
      "Epoch: 41 \tTraining Loss: 0.879049\n",
      "Epoch: 42 \tTraining Loss: 0.876359\n",
      "Epoch: 43 \tTraining Loss: 0.873860\n",
      "Epoch: 44 \tTraining Loss: 0.871525\n",
      "Epoch: 45 \tTraining Loss: 0.869337\n",
      "Epoch: 46 \tTraining Loss: 0.867281\n",
      "Epoch: 47 \tTraining Loss: 0.865341\n",
      "Epoch: 48 \tTraining Loss: 0.863504\n",
      "Epoch: 49 \tTraining Loss: 0.861758\n",
      "Epoch: 50 \tTraining Loss: 0.860090\n",
      "Epoch: 51 \tTraining Loss: 0.858490\n",
      "Epoch: 52 \tTraining Loss: 0.856950\n",
      "Epoch: 53 \tTraining Loss: 0.855461\n",
      "Epoch: 54 \tTraining Loss: 0.854018\n",
      "Epoch: 55 \tTraining Loss: 0.852616\n",
      "Epoch: 56 \tTraining Loss: 0.851251\n",
      "Epoch: 57 \tTraining Loss: 0.849919\n",
      "Epoch: 58 \tTraining Loss: 0.848617\n",
      "Epoch: 59 \tTraining Loss: 0.847343\n",
      "Epoch: 60 \tTraining Loss: 0.846094\n",
      "Epoch: 61 \tTraining Loss: 0.844870\n",
      "Epoch: 62 \tTraining Loss: 0.843669\n",
      "Epoch: 63 \tTraining Loss: 0.842492\n",
      "Epoch: 64 \tTraining Loss: 0.841336\n",
      "Epoch: 65 \tTraining Loss: 0.840202\n",
      "Epoch: 66 \tTraining Loss: 0.839087\n",
      "Epoch: 67 \tTraining Loss: 0.837991\n",
      "Epoch: 68 \tTraining Loss: 0.836913\n",
      "Epoch: 69 \tTraining Loss: 0.835852\n",
      "Epoch: 70 \tTraining Loss: 0.834806\n",
      "Epoch: 71 \tTraining Loss: 0.833775\n",
      "Epoch: 72 \tTraining Loss: 0.832757\n",
      "Epoch: 73 \tTraining Loss: 0.831753\n",
      "Epoch: 74 \tTraining Loss: 0.830762\n",
      "Epoch: 75 \tTraining Loss: 0.829782\n",
      "Epoch: 76 \tTraining Loss: 0.828815\n",
      "Epoch: 77 \tTraining Loss: 0.827859\n",
      "Epoch: 78 \tTraining Loss: 0.826914\n",
      "Epoch: 79 \tTraining Loss: 0.825981\n",
      "Epoch: 80 \tTraining Loss: 0.825059\n",
      "Epoch: 81 \tTraining Loss: 0.824147\n",
      "Epoch: 82 \tTraining Loss: 0.823246\n",
      "Epoch: 83 \tTraining Loss: 0.822356\n",
      "Epoch: 84 \tTraining Loss: 0.821476\n",
      "Epoch: 85 \tTraining Loss: 0.820606\n",
      "Epoch: 86 \tTraining Loss: 0.819745\n",
      "Epoch: 87 \tTraining Loss: 0.818893\n",
      "Epoch: 88 \tTraining Loss: 0.818050\n",
      "Epoch: 89 \tTraining Loss: 0.817215\n",
      "Epoch: 90 \tTraining Loss: 0.816389\n",
      "Epoch: 91 \tTraining Loss: 0.815570\n",
      "Epoch: 92 \tTraining Loss: 0.814758\n",
      "Epoch: 93 \tTraining Loss: 0.813953\n",
      "Epoch: 94 \tTraining Loss: 0.813154\n",
      "Epoch: 95 \tTraining Loss: 0.812362\n",
      "Epoch: 96 \tTraining Loss: 0.811575\n",
      "Epoch: 97 \tTraining Loss: 0.810794\n",
      "Epoch: 98 \tTraining Loss: 0.810017\n",
      "Epoch: 99 \tTraining Loss: 0.809245\n",
      "Epoch: 100 \tTraining Loss: 0.808479\n",
      "Epoch: 101 \tTraining Loss: 0.807718\n",
      "Epoch: 102 \tTraining Loss: 0.806964\n",
      "Epoch: 103 \tTraining Loss: 0.806216\n",
      "Epoch: 104 \tTraining Loss: 0.805473\n",
      "Epoch: 105 \tTraining Loss: 0.804736\n",
      "Epoch: 106 \tTraining Loss: 0.804003\n",
      "Epoch: 107 \tTraining Loss: 0.803276\n",
      "Epoch: 108 \tTraining Loss: 0.802553\n",
      "Epoch: 109 \tTraining Loss: 0.801835\n",
      "Epoch: 110 \tTraining Loss: 0.801120\n",
      "Epoch: 111 \tTraining Loss: 0.800410\n",
      "Epoch: 112 \tTraining Loss: 0.799703\n",
      "Epoch: 113 \tTraining Loss: 0.799000\n",
      "Epoch: 114 \tTraining Loss: 0.798301\n",
      "Epoch: 115 \tTraining Loss: 0.797604\n",
      "Epoch: 116 \tTraining Loss: 0.796910\n",
      "Epoch: 117 \tTraining Loss: 0.796220\n",
      "Epoch: 118 \tTraining Loss: 0.795531\n",
      "Epoch: 119 \tTraining Loss: 0.794846\n",
      "Epoch: 120 \tTraining Loss: 0.794163\n",
      "Epoch: 121 \tTraining Loss: 0.793483\n",
      "Epoch: 122 \tTraining Loss: 0.792806\n",
      "Epoch: 123 \tTraining Loss: 0.792132\n",
      "Epoch: 124 \tTraining Loss: 0.791460\n",
      "Epoch: 125 \tTraining Loss: 0.790791\n",
      "Epoch: 126 \tTraining Loss: 0.790126\n",
      "Epoch: 127 \tTraining Loss: 0.789462\n",
      "Epoch: 128 \tTraining Loss: 0.788802\n",
      "Epoch: 129 \tTraining Loss: 0.788145\n",
      "Epoch: 130 \tTraining Loss: 0.787491\n",
      "Epoch: 131 \tTraining Loss: 0.786839\n",
      "Epoch: 132 \tTraining Loss: 0.786191\n",
      "Epoch: 133 \tTraining Loss: 0.785545\n",
      "Epoch: 134 \tTraining Loss: 0.784902\n",
      "Epoch: 135 \tTraining Loss: 0.784261\n",
      "Epoch: 136 \tTraining Loss: 0.783623\n",
      "Epoch: 137 \tTraining Loss: 0.782988\n",
      "Epoch: 138 \tTraining Loss: 0.782356\n",
      "Epoch: 139 \tTraining Loss: 0.781726\n",
      "Epoch: 140 \tTraining Loss: 0.781099\n",
      "Epoch: 141 \tTraining Loss: 0.780475\n",
      "Epoch: 142 \tTraining Loss: 0.779853\n",
      "Epoch: 143 \tTraining Loss: 0.779234\n",
      "Epoch: 144 \tTraining Loss: 0.778618\n",
      "Epoch: 145 \tTraining Loss: 0.778004\n",
      "Epoch: 146 \tTraining Loss: 0.777393\n",
      "Epoch: 147 \tTraining Loss: 0.776785\n",
      "Epoch: 148 \tTraining Loss: 0.776183\n",
      "Epoch: 149 \tTraining Loss: 0.775586\n",
      "Epoch: 150 \tTraining Loss: 0.774993\n",
      "Epoch: 151 \tTraining Loss: 0.774404\n",
      "Epoch: 152 \tTraining Loss: 0.773818\n",
      "Epoch: 153 \tTraining Loss: 0.773234\n",
      "Epoch: 154 \tTraining Loss: 0.772651\n",
      "Epoch: 155 \tTraining Loss: 0.772064\n",
      "Epoch: 156 \tTraining Loss: 0.771473\n",
      "Epoch: 157 \tTraining Loss: 0.770887\n",
      "Epoch: 158 \tTraining Loss: 0.770308\n",
      "Epoch: 159 \tTraining Loss: 0.769732\n",
      "Epoch: 160 \tTraining Loss: 0.769160\n",
      "Epoch: 161 \tTraining Loss: 0.768591\n",
      "Epoch: 162 \tTraining Loss: 0.768024\n",
      "Epoch: 163 \tTraining Loss: 0.767460\n",
      "Epoch: 164 \tTraining Loss: 0.766898\n",
      "Epoch: 165 \tTraining Loss: 0.766338\n",
      "Epoch: 166 \tTraining Loss: 0.765778\n",
      "Epoch: 167 \tTraining Loss: 0.765217\n",
      "Epoch: 168 \tTraining Loss: 0.764657\n",
      "Epoch: 169 \tTraining Loss: 0.764100\n",
      "Epoch: 170 \tTraining Loss: 0.763546\n",
      "Epoch: 171 \tTraining Loss: 0.762996\n",
      "Epoch: 172 \tTraining Loss: 0.762448\n",
      "Epoch: 173 \tTraining Loss: 0.761903\n",
      "Epoch: 174 \tTraining Loss: 0.761360\n",
      "Epoch: 175 \tTraining Loss: 0.760819\n",
      "Epoch: 176 \tTraining Loss: 0.760281\n",
      "Epoch: 177 \tTraining Loss: 0.759744\n",
      "Epoch: 178 \tTraining Loss: 0.759209\n",
      "Epoch: 179 \tTraining Loss: 0.758674\n",
      "Epoch: 180 \tTraining Loss: 0.758141\n",
      "Epoch: 181 \tTraining Loss: 0.757608\n",
      "Epoch: 182 \tTraining Loss: 0.757077\n",
      "Epoch: 183 \tTraining Loss: 0.756547\n",
      "Epoch: 184 \tTraining Loss: 0.756018\n",
      "Epoch: 185 \tTraining Loss: 0.755490\n",
      "Epoch: 186 \tTraining Loss: 0.754964\n",
      "Epoch: 187 \tTraining Loss: 0.754440\n",
      "Epoch: 188 \tTraining Loss: 0.753918\n",
      "Epoch: 189 \tTraining Loss: 0.753397\n",
      "Epoch: 190 \tTraining Loss: 0.752878\n",
      "Epoch: 191 \tTraining Loss: 0.752361\n",
      "Epoch: 192 \tTraining Loss: 0.751844\n",
      "Epoch: 193 \tTraining Loss: 0.751325\n",
      "Epoch: 194 \tTraining Loss: 0.750804\n",
      "Epoch: 195 \tTraining Loss: 0.750283\n",
      "Epoch: 196 \tTraining Loss: 0.749763\n",
      "Epoch: 197 \tTraining Loss: 0.749247\n",
      "Epoch: 198 \tTraining Loss: 0.748733\n",
      "Epoch: 199 \tTraining Loss: 0.748221\n",
      "Epoch: 200 \tTraining Loss: 0.747711\n"
     ]
    }
   ],
   "source": [
    "# took inspiration from the notebook : https://www.kaggle.com/mishra1993/pytorch-multi-layer-perceptron-mnist\n",
    "# number of epochs to train the model\n",
    "n_epochs = 200\n",
    "\n",
    "# initialize tracker for minimum validation loss\n",
    "valid_loss_min = np.Inf # set initial \"min\" to infinity\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    lstm_model.train() # prep model for training\n",
    "    for data, target in train_loader:\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = lstm_model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    \n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n",
    "        epoch+1, \n",
    "        train_loss,\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a243dcb8",
   "metadata": {},
   "source": [
    "## Accuracy for LSTM model on test set is = 0.62975"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9dd12c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for LSTM model is =  0.62975\n"
     ]
    }
   ],
   "source": [
    "accuracy_on_test = predict(lstm_model,test_loader)\n",
    "print(\"Accuracy for LSTM model is = \",accuracy_on_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e723377e",
   "metadata": {},
   "source": [
    "### `Q — What do you conclude by comparing accuracy values you obtain by GRU, LSTM, and simple RNN.`\n",
    "\n",
    "**Ans —**\n",
    "- I conclude that GRU (63.11%) and LSTM (62.975%) performed better than RNN (57.45%). They performed better because they solve the long-term dependancy/vanishing gradient problem of RNNs and can understand text much better.\n",
    "- All models were trained for 200 epochs\n",
    "- GRU performed slightly better than LSTM by getting  63.11% compared to 62.975%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
